<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Assistant - Realtime API</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Exo+2:wght@300;400;500;600;700&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Exo 2', sans-serif;
            overflow: hidden;
            background: radial-gradient(ellipse at center, #1a1a2e 0%, #16213e 30%, #0f3460 60%, #0c0c0c 100%);
            position: relative;
            width: 100vw;
            height: 100vh;
        }
        
        /* Animated background particles */
        .bg-particles {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            overflow: hidden;
            z-index: 1;
        }
        
        .particle {
            position: absolute;
            background: radial-gradient(circle, rgba(0, 255, 255, 0.6), rgba(255, 0, 255, 0.3), transparent 70%);
            border-radius: 50%;
            animation: float 8s infinite ease-in-out;
            filter: blur(1px);
        }
        
        .particle:nth-child(1) { width: 4px; height: 4px; top: 20%; left: 10%; animation-delay: 0s; }
        .particle:nth-child(2) { width: 6px; height: 6px; top: 80%; left: 20%; animation-delay: 1s; }
        .particle:nth-child(3) { width: 3px; height: 3px; top: 40%; left: 80%; animation-delay: 2s; }
        .particle:nth-child(4) { width: 5px; height: 5px; top: 60%; left: 90%; animation-delay: 3s; }
        .particle:nth-child(5) { width: 4px; height: 4px; top: 10%; left: 60%; animation-delay: 4s; }
        .particle:nth-child(6) { width: 7px; height: 7px; top: 90%; left: 70%; animation-delay: 5s; }
        
        @keyframes float {
            0%, 100% { 
                transform: translateY(0px) translateX(0px) rotate(0deg) scale(1); 
                opacity: 0.8; 
            }
            25% { 
                transform: translateY(-25px) translateX(10px) rotate(90deg) scale(1.2); 
                opacity: 0.6; 
            }
            50% { 
                transform: translateY(-15px) translateX(-5px) rotate(180deg) scale(0.8); 
                opacity: 0.4; 
            }
            75% { 
                transform: translateY(-35px) translateX(15px) rotate(270deg) scale(1.1); 
                opacity: 0.7; 
            }
        }
        
        /* Enhanced Futuristic Orb */
        .orb-container {
            position: relative;
            z-index: 10;
        }
        
        .orb {
            width: 320px;
            height: 320px;
            border-radius: 50%;
            position: relative;
            background: 
                radial-gradient(ellipse at 25% 25%, rgba(0, 255, 255, 0.4) 0%, transparent 50%),
                radial-gradient(ellipse at 75% 75%, rgba(255, 0, 255, 0.3) 0%, transparent 50%),
                radial-gradient(circle at center, #1a1a2e 0%, #16213e 40%, #0f3460 80%, #533a7b 100%);
            box-shadow: 
                0 0 40px rgba(0, 255, 255, 0.4),
                0 0 70px rgba(255, 0, 255, 0.3),
                0 0 100px rgba(139, 92, 246, 0.2),
                inset 0 0 60px rgba(255, 255, 255, 0.05),
                inset 0 0 20px rgba(0, 255, 255, 0.1);
            transition: all 1.2s cubic-bezier(0.25, 0.46, 0.45, 0.94);
            border: 1px solid rgba(0, 255, 255, 0.2);
            backdrop-filter: blur(10px);
            cursor: pointer;
        }
        
        .orb::before {
            content: '';
            position: absolute;
            top: -8px;
            left: -8px;
            right: -8px;
            bottom: -8px;
            border-radius: 50%;
            background: conic-gradient(
                from 0deg, 
                transparent 0%, 
                rgba(0, 255, 255, 0.2) 25%, 
                transparent 50%, 
                rgba(255, 0, 255, 0.2) 75%, 
                transparent 100%
            );
            animation: orbRotate 12s linear infinite;
            z-index: -1;
            filter: blur(2px);
        }
        
        @keyframes orbRotate {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }

        /* IDLE state */
        @keyframes idleBreath {
            0%, 100% { 
                transform: scale(1) rotate(0deg);
                box-shadow: 
                    0 0 40px rgba(0, 255, 255, 0.4),
                    0 0 70px rgba(255, 0, 255, 0.3),
                    0 0 100px rgba(139, 92, 246, 0.2);
            }
            50% { 
                transform: scale(1.03) rotate(1deg);
                box-shadow: 
                    0 0 50px rgba(0, 255, 255, 0.5),
                    0 0 80px rgba(255, 0, 255, 0.4),
                    0 0 110px rgba(139, 92, 246, 0.3);
            }
        }
        .orb.idle {
            animation: idleBreath 8s infinite ease-in-out;
        }
        
        /* LISTENING state */
        @keyframes listeningPulse {
            0%, 100% { 
                transform: scale(1.08) rotate(-1deg);
                box-shadow: 
                    0 0 60px rgba(0, 255, 255, 0.7),
                    0 0 90px rgba(255, 0, 255, 0.5),
                    0 0 120px rgba(139, 92, 246, 0.4);
            }
            50% { 
                transform: scale(1.15) rotate(1deg);
                box-shadow: 
                    0 0 80px rgba(0, 255, 255, 0.9),
                    0 0 110px rgba(255, 0, 255, 0.7),
                    0 0 140px rgba(139, 92, 246, 0.5);
            }
        }
        .orb.listening {
            animation: listeningPulse 1.5s infinite ease-in-out;
            background: 
                radial-gradient(ellipse at 25% 25%, rgba(0, 255, 255, 0.6) 0%, transparent 50%),
                radial-gradient(ellipse at 75% 75%, rgba(255, 0, 255, 0.4) 0%, transparent 50%),
                radial-gradient(circle at center, #1a1a2e 0%, #16213e 30%, #0f3460 70%, #3b82f6 100%);
        }

        /* THINKING state */
        @keyframes thinkingPulse {
            0%, 100% { transform: scale(1.02) rotate(-0.5deg); }
            50% { transform: scale(1.06) rotate(0.5deg); }
        }
        .orb.thinking {
            background: 
                radial-gradient(ellipse at 35% 35%, rgba(255, 0, 255, 0.4) 0%, transparent 50%),
                radial-gradient(ellipse at 65% 65%, rgba(0, 255, 255, 0.3) 0%, transparent 50%),
                radial-gradient(circle at center, #1a1a2e 0%, #16213e 30%, #533a7b 70%, #8b5cf6 100%);
            animation: thinkingPulse 2s ease-in-out infinite;
        }

        /* SPEAKING state */
        @keyframes speakingWaves {
            0%, 100% { 
                transform: scale(1.04) rotate(-1deg);
                box-shadow: 
                    0 0 50px rgba(255, 255, 0, 0.6),
                    0 0 80px rgba(255, 0, 255, 0.4),
                    0 0 110px rgba(0, 255, 255, 0.3);
            }
            50% { 
                transform: scale(1.12) rotate(1deg);
                box-shadow: 
                    0 0 80px rgba(255, 255, 0, 0.9),
                    0 0 110px rgba(255, 0, 255, 0.7),
                    0 0 140px rgba(0, 255, 255, 0.5);
            }
        }
        .orb.speaking {
            animation: speakingWaves 1.2s infinite ease-in-out;
            background: 
                radial-gradient(ellipse at 30% 30%, rgba(255, 255, 0, 0.5) 0%, transparent 50%),
                radial-gradient(ellipse at 70% 70%, rgba(255, 0, 255, 0.3) 0%, transparent 50%),
                radial-gradient(circle at center, #1a1a2e 0%, #16213e 20%, #ffaa00 60%, #8b5cf6 100%);
        }
        
        /* Status text */
        .status-text {
            position: absolute;
            bottom: -80px;
            width: 100%;
            text-align: center;
            font-family: 'Orbitron', monospace;
            font-size: 1.2rem;
            font-weight: 400;
            color: transparent;
            background: linear-gradient(
                135deg, 
                rgba(0, 255, 255, 0.8) 0%, 
                rgba(255, 0, 255, 0.6) 50%, 
                rgba(255, 255, 0, 0.4) 100%
            );
            background-clip: text;
            -webkit-background-clip: text;
            text-shadow: 
                0 0 15px rgba(0, 255, 255, 0.3),
                0 2px 10px rgba(255, 0, 255, 0.2);
            transition: all 0.6s cubic-bezier(0.25, 0.46, 0.45, 0.94);
            letter-spacing: 1.5px;
            animation: textGlow 3s ease-in-out infinite;
        }
        
        @keyframes textGlow {
            0%, 100% { 
                opacity: 0.8;
                transform: translateY(0px);
            }
            50% { 
                opacity: 1;
                transform: translateY(-2px);
            }
        }

        /* Touch indicator */
        .touch-indicator {
            position: absolute;
            top: 20px;
            right: 20px;
            font-family: 'Orbitron', monospace;
            font-size: 0.9rem;
            color: rgba(0, 255, 255, 0.7);
            z-index: 20;
            text-shadow: 0 0 10px rgba(0, 255, 255, 0.5);
        }

        /* Mode indicator */
        .mode-indicator {
            position: absolute;
            bottom: 20px;
            left: 20px;
            font-family: 'Orbitron', monospace;
            font-size: 0.8rem;
            color: rgba(0, 255, 255, 0.6);
            z-index: 20;
            text-shadow: 0 0 8px rgba(0, 255, 255, 0.4);
            opacity: 0;
            transition: opacity 0.5s ease-in-out;
        }
        
        .mode-indicator.visible {
            opacity: 1;
        }
        
        @keyframes pulseHint {
            0%, 100% { opacity: 0.7; }
            50% { opacity: 1; }
        }
        
        .pulse-hint {
            animation: pulseHint 2s infinite ease-in-out;
        }
    </style>
</head>
<body>
    <!-- Animated Background -->
    <div class="bg-particles">
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
    </div>
    
    <!-- Touch Indicator -->
    <div class="touch-indicator pulse-hint">
        TAP TO ACTIVATE
    </div>
    
    <!-- System Mode Indicator -->
    <div class="mode-indicator" id="mode-indicator">
        REALTIME API MODE
    </div>
    
    <!-- Main Assistant Interface -->
    <div class="relative flex items-center justify-center w-full h-full">
        <div class="orb-container">
            <div id="persona-orb" class="orb idle"></div>
            <div id="status-text" class="status-text">TAP TO START</div>
        </div>
    </div>

    <script>
        /*
         * AI Voice Assistant - OpenAI Realtime API Implementation
         * 
         * Pure Realtime API implementation with server-side proxy for authentication.
         * No backup systems - uses only OpenAI's real-time voice-to-voice API.
         */

        const personaOrb = document.getElementById('persona-orb');
        const statusText = document.getElementById('status-text');
        const touchIndicator = document.querySelector('.touch-indicator');
        const modeIndicator = document.getElementById('mode-indicator');

        // Realtime API Configuration - Auto-detect environment
        const getProxyURL = () => {
            // Check if we're on GitHub Pages (shucho.space)
            if (window.location.hostname === 'shucho.space' || window.location.hostname.includes('github.io')) {
                // GitHub Codespace URL - Fixed for current deployment
                return 'wss://laughing-train-g4pxx5r5rg739667-3001.app.github.dev/realtime';
            }
            // Local development
            return 'ws://localhost:3001/realtime';
        };
        
        const PROXY_URL = getProxyURL();
        const REALTIME_MODEL = 'gpt-4o-realtime-preview-2024-10-01';
        
        console.log('üîß Using proxy URL:', PROXY_URL);
        
        // State variables
        let realtimeSocket = null;
        let audioStream = null;
        let audioContext = null;
        let microphoneSource = null;
        let isConnected = false;
        let isSessionReady = false;
        let isActivated = false;
        let isSpeaking = false;
        let connectionAttempts = 0;
        let maxRetries = 3;

        // Initialize audio context and microphone
        async function initializeAudio() {
            try {
                console.log('üé§ Initializing audio...');
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                audioStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 24000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    } 
                });
                console.log('‚úÖ Audio initialized');
                return true;
            } catch (error) {
                console.error('‚ùå Audio initialization failed:', error);
                setPersonaState('idle', 'Microphone access denied');
                return false;
            }
        }

        // Connect to OpenAI Realtime API through proxy
        async function connectToRealtimeAPI() {
            try {
                console.log('üîÑ Connecting to OpenAI Realtime API...');
                setPersonaState('idle', 'CONNECTING...');
                
                const wsUrl = `${PROXY_URL}?model=${REALTIME_MODEL}`;
                realtimeSocket = new WebSocket(wsUrl);
                
                realtimeSocket.onopen = () => {
                    console.log('‚úÖ Connected to Realtime API');
                    
                    // Test with g711_ulaw format explicitly (OpenAI documented format)
                    setTimeout(() => {
                        console.log('üîß Testing with g711_ulaw audio format...');
                        realtimeSocket.send(JSON.stringify({
                            type: 'session.update',
                            session: {
                                input_audio_format: 'g711_ulaw',
                                output_audio_format: 'g711_ulaw'
                            }
                        }));
                    }, 500); // Shorter delay
                    
                    setPersonaState('idle', 'Configuring g711_ulaw...');
                    isConnected = true;

                    // Show mode indicator
                    modeIndicator.classList.add('visible');
                    setTimeout(() => {
                        modeIndicator.classList.remove('visible');
                    }, 3000);
                };
                
                realtimeSocket.onmessage = handleRealtimeMessage;
                
                realtimeSocket.onerror = (error) => {
                    console.error('‚ùå Realtime WebSocket error:', error);
                    if (PROXY_URL.includes('your-proxy-server')) {
                        setPersonaState('idle', 'Server not deployed yet');
                    } else {
                        setPersonaState('idle', 'Connection failed');
                    }
                };
                
                realtimeSocket.onclose = (event) => {
                    console.log('üîå Realtime WebSocket closed:', event.code, event.reason);
                    if (PROXY_URL.includes('your-proxy-server')) {
                        setPersonaState('idle', 'Deploy server first');
                    } else {
                        setPersonaState('idle', 'Disconnected');
                    }
                    isConnected = false;
                };
                
            } catch (error) {
                console.error('‚ùå Failed to connect to Realtime API:', error);
                setPersonaState('idle', 'Connection error');
            }
        }

        // Start real-time audio streaming with modern Web Audio API
        function startRealtimeAudio() {
            if (!isConnected || !isSessionReady || !audioStream || !audioContext) {
                console.log('‚ö†Ô∏è Not ready for audio streaming:', { isConnected, isSessionReady, hasAudioStream: !!audioStream, hasAudioContext: !!audioContext });
                return;
            }
            
            console.log('üé§ Starting real-time audio streaming...');
            
            const source = audioContext.createMediaStreamSource(audioStream);
            
            // Use modern approach - we'll process audio in chunks
            let isProcessing = false;
            
            const processAudioChunk = async () => {
                if (!isConnected || !realtimeSocket || realtimeSocket.readyState !== WebSocket.OPEN || isProcessing) {
                    requestAnimationFrame(processAudioChunk);
                    return;
                }
                
                isProcessing = true;
                
                try {
                    // Create a short audio buffer to capture current audio
                    const bufferSize = 4096;
                    const mediaRecorder = new MediaRecorder(audioStream, {
                        mimeType: 'audio/webm;codecs=opus'
                    });
                    
                    // For now, we'll continue using ScriptProcessor but with better error handling
                    // TODO: Implement proper AudioWorkletNode in future update
                    
                } catch (error) {
                    console.error('‚ùå Audio processing error:', error);
                }
                
                isProcessing = false;
                requestAnimationFrame(processAudioChunk);
            };
            
            // For now, keep the working ScriptProcessor approach but add better error handling
            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            
            processor.onaudioprocess = (event) => {
                if (!isConnected || !isSessionReady || !realtimeSocket || realtimeSocket.readyState !== WebSocket.OPEN) {
                    return;
                }
                
                try {
                    const inputData = event.inputBuffer.getChannelData(0);
                    
                    // Convert to g711_ulaw format
                    const ulawData = new Uint8Array(inputData.length);
                    
                    for (let i = 0; i < inputData.length; i++) {
                        // Convert float32 (-1 to 1) to 16-bit PCM first
                        const pcm16 = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
                        // Convert PCM16 to Œº-law
                        ulawData[i] = linearToUlaw(pcm16);
                    }
                    
                    // Send audio data to Realtime API in g711_ulaw format
                    realtimeSocket.send(JSON.stringify({
                        type: 'input_audio_buffer.append',
                        audio: arrayBufferToBase64(ulawData.buffer)
                    }));
                } catch (error) {
                    console.error('‚ùå Audio processing error:', error);
                }
            };
            
            source.connect(processor);
            processor.connect(audioContext.destination);
            
            microphoneSource = { source, processor };
            console.log('‚úÖ Real-time audio streaming active (ScriptProcessor - will migrate to AudioWorklet)');
        }

        // Handle Realtime API messages
        function handleRealtimeMessage(event) {
            // Handle both text and binary messages
            let message;
            try {
                if (typeof event.data === 'string') {
                    message = JSON.parse(event.data);
                } else {
                    // Binary data (audio) - convert to text first or handle separately
                    console.log('üì¶ Received binary data:', event.data);
                    return; // Skip binary messages for now
                }
            } catch (error) {
                console.error('‚ùå Failed to parse WebSocket message:', error, event.data);
                return;
            }
            
            console.log('üì® Realtime message:', message.type);

            switch (message.type) {
                case 'session.created':
                    console.log('‚úÖ Realtime session created');
                    connectionAttempts = 0; // Reset retry counter on success
                    // Don't start audio yet - wait for audio configuration
                    break;
                
                case 'session.updated':
                    console.log('‚úÖ Session audio configuration updated to g711_ulaw');
                    isSessionReady = true;
                    setPersonaState('idle', 'Ready');
                    // Now it's safe to start audio streaming
                    startRealtimeAudio();
                    break;
                
                case 'input_audio_buffer.speech_started':
                    console.log('üé§ Speech started');
                    setPersonaState('listening', 'Listening...');
                    break;
                
                case 'input_audio_buffer.speech_stopped':
                    console.log('üé§ Speech stopped');
                    setPersonaState('thinking', 'Processing...');
                    break;
                
                case 'response.audio.delta':
                    if (message.delta) {
                        playAudioDelta(message.delta);
                    }
                    break;
                
                case 'response.audio.done':
                    console.log('üîä Audio response complete');
                    setPersonaState('idle', 'Ready');
                    isSpeaking = false;
                    break;
                
                case 'response.done':
                    console.log('‚úÖ Response complete');
                    setPersonaState('idle', 'Ready');
                    break;
                
                case 'error':
                    console.error('‚ùå Realtime API error:', message.error);
                    
                    // Handle server errors with retry logic
                    if (message.error?.type === 'server_error' && connectionAttempts < maxRetries) {
                        connectionAttempts++;
                        console.log(`üîÑ Retrying connection (${connectionAttempts}/${maxRetries}) in 2 seconds...`);
                        setPersonaState('idle', `Retrying... (${connectionAttempts}/${maxRetries})`);
                        
                        setTimeout(() => {
                            if (realtimeSocket) {
                                realtimeSocket.close();
                            }
                            connectToRealtimeAPI();
                        }, 2000);
                    } else {
                        setPersonaState('idle', 'Error occurred');
                        connectionAttempts = 0;
                    }
                    break;
            }
        }

        // Play audio response from Realtime API
        let audioQueue = [];
        let isPlayingAudio = false;
        
        function playAudioDelta(base64Audio) {
            if (!isSpeaking) {
                setPersonaState('speaking', 'Speaking...');
                isSpeaking = true;
            }
            
            audioQueue.push(base64Audio);
            if (!isPlayingAudio) {
                processAudioQueue();
            }
        }
        
        async function processAudioQueue() {
            if (audioQueue.length === 0) {
                isPlayingAudio = false;
                return;
            }
            
            isPlayingAudio = true;
            
            const base64Audio = audioQueue.shift();
            try {
                const audioData = base64ToArrayBuffer(base64Audio);
                
                // Decode g711_ulaw format from OpenAI
                const ulawBytes = new Uint8Array(audioData);
                const pcmData = new Int16Array(ulawBytes.length);
                
                for (let i = 0; i < ulawBytes.length; i++) {
                    pcmData[i] = ulawToLinear(ulawBytes[i]);
                }
                
                // Create audio buffer from decoded PCM data
                const audioBuffer = audioContext.createBuffer(1, pcmData.length, 24000);
                const channelData = audioBuffer.getChannelData(0);
                
                for (let i = 0; i < pcmData.length; i++) {
                    channelData[i] = pcmData[i] / 32768; // Convert to float32
                }
                
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                
                source.onended = () => {
                    processAudioQueue();
                };
                
                source.start();
            } catch (error) {
                console.error('‚ùå Audio playback error:', error);
                processAudioQueue(); // Continue with next chunk
            }
        }

        // Utility functions
        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.byteLength; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }
        
        // Œº-law encoding (OpenAI default format)
        function linearToUlaw(pcm) {
            const BIAS = 0x84;
            const CLIP = 32635;
            
            let sign = (pcm >> 8) & 0x80;
            if (sign) pcm = -pcm;
            if (pcm > CLIP) pcm = CLIP;
            pcm += BIAS;
            
            let exponent = 7;
            let exponentMask = 0x4000;
            for (let i = 0; i < 8; i++) {
                if ((pcm & exponentMask) !== 0) break;
                exponent--;
                exponentMask >>= 1;
            }
            
            const mantissa = (pcm >> (exponent + 3)) & 0x0F;
            const ulaw = ~(sign | (exponent << 4) | mantissa);
            
            return ulaw & 0xFF;
        }
        
        // Œº-law decoding (OpenAI default format)
        function ulawToLinear(ulaw) {
            const BIAS = 0x84;
            
            ulaw = ~ulaw;
            const sign = ulaw & 0x80;
            const exponent = (ulaw >> 4) & 0x07;
            const mantissa = ulaw & 0x0F;
            
            let sample = (mantissa << (exponent + 3)) + BIAS;
            if (exponent > 0) sample += (1 << (exponent + 2));
            
            return sign ? -sample : sample;
        }
        
        function base64ToArrayBuffer(base64) {
            const binaryString = atob(base64);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }

        // Visual state management
        function setPersonaState(state, text) {
            personaOrb.className = 'orb';
            personaOrb.classList.add(state);
            statusText.textContent = text ? text.toUpperCase() : '';
            
            if (state !== 'idle' || text !== 'CONNECTING...') {
                touchIndicator.style.display = 'none';
            }
        }

        // Activation handler
        async function activateAssistant() {
            if (!isActivated) {
                isActivated = true;
                touchIndicator.textContent = 'ACTIVATING...';
                touchIndicator.classList.remove('pulse-hint');
                
                setPersonaState('idle', 'INITIALIZING...');
                
                const audioInitialized = await initializeAudio();
                if (audioInitialized) {
                    await connectToRealtimeAPI();
                    setTimeout(() => {
                        touchIndicator.style.display = 'none';
                    }, 2000);
                } else {
                    setPersonaState('idle', 'MICROPHONE ERROR');
                }
            }
        }
        
        // Event listeners
        document.body.addEventListener('click', activateAssistant, { once: true });
        document.body.addEventListener('touchstart', activateAssistant, { once: true });
        personaOrb.addEventListener('click', activateAssistant);
        personaOrb.addEventListener('touchstart', activateAssistant);

    </script>
</body>
</html>