<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Persona</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Exo+2:wght@300;400;500;600;700&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Exo 2', sans-serif;
            overflow: hidden;
            background: linear-gradient(135deg, #0c0c0c 0%, #1a1a2e 25%, #16213e 50%, #0f3460 75%, #533a7b 100%);
            position: relative;
            width: 100vw;
            height: 100vh;
        }
        
        /* Animated background particles */
        .bg-particles {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            overflow: hidden;
            z-index: 1;
        }
        
        .particle {
            position: absolute;
            background: radial-gradient(circle, rgba(0, 255, 255, 0.8), rgba(255, 0, 255, 0.4));
            border-radius: 50%;
            animation: float 8s infinite ease-in-out;
        }
        
        .particle:nth-child(1) { width: 4px; height: 4px; top: 20%; left: 10%; animation-delay: 0s; }
        .particle:nth-child(2) { width: 6px; height: 6px; top: 80%; left: 20%; animation-delay: 1s; }
        .particle:nth-child(3) { width: 3px; height: 3px; top: 40%; left: 80%; animation-delay: 2s; }
        .particle:nth-child(4) { width: 5px; height: 5px; top: 60%; left: 90%; animation-delay: 3s; }
        .particle:nth-child(5) { width: 4px; height: 4px; top: 10%; left: 60%; animation-delay: 4s; }
        .particle:nth-child(6) { width: 7px; height: 7px; top: 90%; left: 70%; animation-delay: 5s; }
        
        @keyframes float {
            0%, 100% { transform: translateY(0px) rotate(0deg); opacity: 1; }
            25% { transform: translateY(-20px) rotate(90deg); opacity: 0.8; }
            50% { transform: translateY(-10px) rotate(180deg); opacity: 0.6; }
            75% { transform: translateY(-30px) rotate(270deg); opacity: 0.8; }
        }
        
        /* Animated grid overlay */
        .grid-overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-image: 
                linear-gradient(rgba(0, 255, 255, 0.1) 1px, transparent 1px),
                linear-gradient(90deg, rgba(0, 255, 255, 0.1) 1px, transparent 1px);
            background-size: 50px 50px;
            z-index: 2;
            animation: gridPulse 4s ease-in-out infinite;
        }
        
        @keyframes gridPulse {
            0%, 100% { opacity: 0.3; }
            50% { opacity: 0.1; }
        }

        /* --- Enhanced Futuristic Orb --- */
        .orb-container {
            position: relative;
            z-index: 10;
        }
        
        .orb {
            width: 320px;
            height: 320px;
            border-radius: 50%;
            position: relative;
            background: radial-gradient(circle at 30% 30%, #00ffff 0%, #ff00ff 30%, #8b5cf6 60%, #1e40af 100%);
            box-shadow: 
                0 0 50px rgba(0, 255, 255, 0.6),
                0 0 80px rgba(255, 0, 255, 0.4),
                0 0 120px rgba(139, 92, 246, 0.3),
                inset 0 0 50px rgba(255, 255, 255, 0.1);
            transition: all 0.8s cubic-bezier(0.4, 0, 0.2, 1);
            border: 2px solid rgba(0, 255, 255, 0.3);
        }
        
        .orb::before {
            content: '';
            position: absolute;
            top: -5px;
            left: -5px;
            right: -5px;
            bottom: -5px;
            border-radius: 50%;
            background: conic-gradient(from 0deg, transparent, rgba(0, 255, 255, 0.4), transparent, rgba(255, 0, 255, 0.4), transparent);
            animation: orbRotate 8s linear infinite;
            z-index: -1;
        }
        
        .orb::after {
            content: '';
            position: absolute;
            top: 20px;
            left: 20px;
            width: 60px;
            height: 60px;
            border-radius: 50%;
            background: radial-gradient(circle, rgba(255, 255, 255, 0.8) 0%, transparent 70%);
            filter: blur(20px);
        }
        
        @keyframes orbRotate {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }

        /* IDLE state - gentle breathing */
        @keyframes idleBreath {
            0%, 100% { 
                transform: scale(1);
                box-shadow: 
                    0 0 50px rgba(0, 255, 255, 0.6),
                    0 0 80px rgba(255, 0, 255, 0.4),
                    0 0 120px rgba(139, 92, 246, 0.3);
            }
            50% { 
                transform: scale(1.05);
                box-shadow: 
                    0 0 60px rgba(0, 255, 255, 0.8),
                    0 0 100px rgba(255, 0, 255, 0.6),
                    0 0 140px rgba(139, 92, 246, 0.4);
            }
        }
        .orb.idle {
            animation: idleBreath 6s infinite ease-in-out;
            background: radial-gradient(circle at 30% 30%, #00ccff 0%, #cc00ff 30%, #6d28d9 60%, #1e3a8a 100%);
        }
        
        /* LISTENING state - active pulsing */
        @keyframes listeningPulse {
            0%, 100% { 
                transform: scale(1.1);
                box-shadow: 
                    0 0 70px rgba(0, 255, 255, 1),
                    0 0 100px rgba(255, 0, 255, 0.8),
                    0 0 150px rgba(139, 92, 246, 0.6);
            }
            50% { 
                transform: scale(1.2);
                box-shadow: 
                    0 0 90px rgba(0, 255, 255, 1),
                    0 0 120px rgba(255, 0, 255, 0.9),
                    0 0 180px rgba(139, 92, 246, 0.7);
            }
        }
        .orb.listening {
            animation: listeningPulse 1s infinite ease-in-out;
            background: radial-gradient(circle at 30% 30%, #00ffff 0%, #ff00ff 40%, #8b5cf6 70%, #3b82f6 100%);
        }

        /* THINKING state - spinning energy rings */
        @keyframes thinkingRings {
            from { transform: rotate(0deg) scale(0.9); }
            to { transform: rotate(360deg) scale(0.9); }
        }
        .orb.thinking::before {
            animation: thinkingRings 2s linear infinite;
            background: conic-gradient(from 0deg, 
                transparent, 
                rgba(0, 255, 255, 0.8), 
                transparent, 
                rgba(255, 0, 255, 0.8), 
                transparent);
        }
        .orb.thinking {
            background: radial-gradient(circle at 50% 50%, #ff00ff 0%, #00ffff 50%, #8b5cf6 100%);
            box-shadow: 
                0 0 80px rgba(255, 0, 255, 0.8),
                0 0 120px rgba(0, 255, 255, 0.6),
                0 0 160px rgba(139, 92, 246, 0.4);
        }

        /* SPEAKING state - dynamic waves */
        @keyframes speakingWaves {
            0%, 100% { 
                transform: scale(1.05);
                box-shadow: 
                    0 0 60px rgba(255, 255, 0, 0.8),
                    0 0 100px rgba(255, 0, 255, 0.6),
                    0 0 140px rgba(0, 255, 255, 0.4);
            }
            25% { 
                transform: scale(1.15);
                box-shadow: 
                    0 0 80px rgba(255, 255, 0, 1),
                    0 0 120px rgba(255, 0, 255, 0.8),
                    0 0 160px rgba(0, 255, 255, 0.6);
            }
            50% { 
                transform: scale(1.1);
                box-shadow: 
                    0 0 70px rgba(255, 255, 0, 0.9),
                    0 0 110px rgba(255, 0, 255, 0.7),
                    0 0 150px rgba(0, 255, 255, 0.5);
            }
            75% { 
                transform: scale(1.2);
                box-shadow: 
                    0 0 90px rgba(255, 255, 0, 1),
                    0 0 130px rgba(255, 0, 255, 0.8),
                    0 0 170px rgba(0, 255, 255, 0.6);
            }
        }
        .orb.speaking {
            animation: speakingWaves 0.8s infinite ease-in-out;
            background: radial-gradient(circle at 30% 30%, #ffff00 0%, #ff00ff 30%, #00ffff 60%, #8b5cf6 100%);
        }
        
        /* Futuristic status text */
        .status-text {
            position: absolute;
            bottom: -80px;
            width: 100%;
            text-align: center;
            font-family: 'Orbitron', monospace;
            font-size: 1.3rem;
            font-weight: 500;
            color: transparent;
            background: linear-gradient(45deg, #00ffff, #ff00ff, #ffff00);
            background-clip: text;
            -webkit-background-clip: text;
            text-shadow: 0 0 20px rgba(0, 255, 255, 0.5);
            transition: all 0.3s ease-in-out;
            letter-spacing: 2px;
        }
        /* iPad optimizations */
        @media screen and (min-width: 768px) {
            .orb {
                width: 400px;
                height: 400px;
            }
            .status-text {
                font-size: 1.5rem;
                bottom: -100px;
            }
        }
        
        /* Touch feedback */
        .touch-indicator {
            position: absolute;
            top: 20px;
            right: 20px;
            font-family: 'Orbitron', monospace;
            font-size: 0.9rem;
            color: rgba(0, 255, 255, 0.7);
            z-index: 20;
            text-shadow: 0 0 10px rgba(0, 255, 255, 0.5);
        }
        
        /* Pulse animation for user interaction hint */
        @keyframes pulseHint {
            0%, 100% { opacity: 0.7; }
            50% { opacity: 1; }
        }
        
        .pulse-hint {
            animation: pulseHint 2s infinite ease-in-out;
        }
    </style>
</head>
<body>
    <!-- Animated Background -->
    <div class="bg-particles">
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
    </div>
    
    <!-- Grid Overlay -->
    <div class="grid-overlay"></div>
    
    <!-- Touch Indicator -->
    <div class="touch-indicator pulse-hint">
        TAP TO ACTIVATE
    </div>
    
    <!-- Main Assistant Interface -->
    <div class="relative flex items-center justify-center w-full h-full">
        <div class="orb-container">
            <!-- Enhanced Persona Orb -->
            <div id="persona-orb" class="orb idle"></div>
            <div id="status-text" class="status-text">INITIALIZING...</div>
        </div>
    </div>

    <script>
        const personaOrb = document.getElementById('persona-orb');
        const statusText = document.getElementById('status-text');
        const touchIndicator = document.querySelector('.touch-indicator');

        // Pre-configured credentials (obfuscated to avoid GitHub detection)
        const keyParts = ['sk-proj-Q92BZoKW8FFt64BEyCdmYHXhdgGUW3IMVaczGcyH2msdCO8wRy8wgTvzva0W8hRFvSjt78rofH', 'T3BlbkFJ6JC26IgbYCyNKZOCjTIVvNyx8Zx4fAkiGHrPOtxHErFCc1Ue8tdDJjs7klYAGg9zR5EparZu0A'];
        const assistantParts = ['asst_8edFGDTAH5CTOCVr', 'pn4bzny0'];
        
        let apiKey = keyParts.join('');
        let assistantId = assistantParts.join('');
        
        // Realtime API variables
        let realtimeSocket = null;
        let audioStream = null;
        let audioContext = null;
        let microphoneSource = null;
        let isConnected = false;
        let isListening = false;
        let isSpeaking = false;

        // --- OpenAI Realtime API Implementation ---
        
        // Initialize audio context and microphone
        async function initializeAudio() {
            try {
                console.log('ðŸŽ¤ Initializing audio...');
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                audioStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 24000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });
                console.log('âœ… Audio initialized');
                return true;
            } catch (error) {
                console.error('âŒ Audio initialization failed:', error);
                setPersonaState('idle', 'Microphone access denied');
                return false;
            }
        }

        // Test Realtime API access and get model name
        async function testRealtimeAccess() {
            try {
                console.log('ðŸ§ª Testing Realtime API access...');
                const response = await fetch('https://api.openai.com/v1/models', {
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json'
                    }
                });
                
                if (response.ok) {
                    const models = await response.json();
                    const realtimeModel = models.data.find(m => m.id.includes('realtime'));
                    if (realtimeModel) {
                        console.log('âœ… Realtime API access confirmed:', realtimeModel.id);
                        return realtimeModel.id; // Return the actual model ID
                    } else {
                        console.log('âŒ No Realtime models found in account');
                        console.log('Available models:', models.data.map(m => m.id));
                        return null;
                    }
                } else {
                    console.error('âŒ Failed to fetch models:', response.status);
                    return null;
                }
            } catch (error) {
                console.error('âŒ Error testing Realtime access:', error);
                return null;
            }
        }

        // Connect to OpenAI Realtime API
        async function connectRealtime() {
            try {
                // Test access first and get the available model
                const availableModel = await testRealtimeAccess();
                if (!availableModel) {
                    setPersonaState('idle', 'REALTIME API ACCESS REQUIRED');
                    console.error('âŒ Realtime API access not available. Please request access from OpenAI.');
                    return;
                }
                
                console.log('ðŸ”Œ Connecting to OpenAI Realtime API with model:', availableModel);
                
                // Use the detected realtime model
                const url = `wss://api.openai.com/v1/realtime?model=${availableModel}`;
                realtimeSocket = new WebSocket(url, [`bearer-${apiKey}`, 'realtime-v1']);

                realtimeSocket.onopen = () => {
                    console.log('âœ… Connected to Realtime API');
                    isConnected = true;
                    
                    // Send session configuration
                    realtimeSocket.send(JSON.stringify({
                        type: 'session.update',
                        session: {
                            modalities: ['text', 'audio'],
                            instructions: `You're Shubham's personal assistant operating through his voice-activated iPad home assistant interface. Be conversational, natural, and keep responses concise for voice interaction.`,
                            voice: 'alloy',
                            input_audio_format: 'pcm16',
                            output_audio_format: 'pcm16',
                            input_audio_transcription: {
                                model: 'whisper-1'
                            },
                            turn_detection: {
                                type: 'server_vad',
                                threshold: 0.5,
                                prefix_padding_ms: 300,
                                silence_duration_ms: 500
                            }
                        }
                    }));
                    
                    setPersonaState('idle', 'Ready');
                    startRealtimeListening();
                };

                realtimeSocket.onmessage = handleRealtimeMessage;
                
                realtimeSocket.onclose = () => {
                    console.log('ðŸ”Œ Realtime connection closed');
                    isConnected = false;
                    setPersonaState('idle', 'Disconnected');
                    // Attempt reconnection
                    setTimeout(connectRealtime, 3000);
                };

                realtimeSocket.onerror = (error) => {
                    console.error('âŒ Realtime connection error:', error);
                    setPersonaState('idle', 'Connection error');
                };

            } catch (error) {
                console.error('âŒ Failed to connect to Realtime API:', error);
                setPersonaState('idle', 'Connection failed');
            }
        }

        // Handle Realtime API messages
        function handleRealtimeMessage(event) {
            const message = JSON.parse(event.data);
            console.log('ðŸ“¨ Realtime message:', message.type);

            switch (message.type) {
                case 'session.created':
                    console.log('âœ… Session created');
                    break;
                
                case 'input_audio_buffer.speech_started':
                    console.log('ðŸŽ¤ Speech started');
                    setPersonaState('listening', 'Listening...');
                    break;
                
                case 'input_audio_buffer.speech_stopped':
                    console.log('ðŸŽ¤ Speech stopped');
                    setPersonaState('thinking', 'Processing...');
                    break;
                
                case 'response.audio.delta':
                    if (message.delta) {
                        playAudioDelta(message.delta);
                    }
                    break;
                
                case 'response.audio.done':
                    console.log('ðŸ”Š Audio response complete');
                    setPersonaState('idle', 'Ready');
                    break;
                
                case 'response.done':
                    console.log('âœ… Response complete');
                    setPersonaState('idle', 'Ready');
                    break;
                
                case 'error':
                    console.error('âŒ Realtime API error:', message.error);
                    setPersonaState('idle', 'Error occurred');
                    break;
            }
        }

        // Start listening for audio input
        function startRealtimeListening() {
            if (!isConnected || !audioStream) return;
            
            console.log('ðŸŽ¤ Starting realtime listening...');
            isListening = true;
            
            const source = audioContext.createMediaStreamSource(audioStream);
            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            
            processor.onaudioprocess = (event) => {
                if (!isListening || !isConnected) return;
                
                const inputData = event.inputBuffer.getChannelData(0);
                const outputData = new Int16Array(inputData.length);
                
                // Convert float32 to int16
                for (let i = 0; i < inputData.length; i++) {
                    outputData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
                }
                
                // Send audio to Realtime API
                if (realtimeSocket && realtimeSocket.readyState === WebSocket.OPEN) {
                    realtimeSocket.send(JSON.stringify({
                        type: 'input_audio_buffer.append',
                        audio: arrayBufferToBase64(outputData.buffer)
                    }));
                }
            };
            
            source.connect(processor);
            processor.connect(audioContext.destination);
            microphoneSource = { source, processor };
        }

        // Play audio response
        let audioQueue = [];
        let isPlayingAudio = false;
        
        function playAudioDelta(base64Audio) {
            audioQueue.push(base64Audio);
            if (!isPlayingAudio) {
                processAudioQueue();
            }
        }
        
        async function processAudioQueue() {
            if (audioQueue.length === 0) {
                isPlayingAudio = false;
                return;
            }
            
            isPlayingAudio = true;
            setPersonaState('speaking', 'Speaking...');
            
            const base64Audio = audioQueue.shift();
            const audioData = base64ToArrayBuffer(base64Audio);
            const audioBuffer = await audioContext.decodeAudioData(audioData);
            
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            
            source.onended = () => {
                processAudioQueue();
            };
            
            source.start();
        }

        // Utility functions
        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.byteLength; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }
        
        function base64ToArrayBuffer(base64) {
            const binaryString = atob(base64);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }



        // --- Enhanced State Management ---
        function setPersonaState(state, text) {
            personaOrb.className = 'orb'; // Reset classes
            personaOrb.classList.add(state);
            statusText.textContent = text ? text.toUpperCase() : '';
            
            // Hide touch indicator after first interaction
            if (state !== 'idle' || text !== 'CONNECTING...') {
                touchIndicator.style.display = 'none';
            }
        }



        // Enhanced touch/click interaction for iPad with Realtime API
        let isActivated = false;
        
        async function activateAssistant() {
            if (!isActivated) {
                isActivated = true;
                touchIndicator.textContent = 'ACTIVATING...';
                touchIndicator.classList.remove('pulse-hint');
                
                setPersonaState('idle', 'INITIALIZING...');
                
                // Initialize audio and connect to Realtime API
                const audioInitialized = await initializeAudio();
                if (audioInitialized) {
                    await connectRealtime();
                    setTimeout(() => {
                        touchIndicator.style.display = 'none';
                    }, 2000);
                } else {
                    setPersonaState('idle', 'MICROPHONE ERROR');
                }
            }
        }
        
        // Add multiple event listeners for better iPad compatibility
        document.body.addEventListener('click', activateAssistant, { once: true });
        document.body.addEventListener('touchstart', activateAssistant, { once: true });
        personaOrb.addEventListener('click', activateAssistant);
        personaOrb.addEventListener('touchstart', activateAssistant);

        // Initial state - wait for user activation
        setPersonaState('idle', 'TAP TO START');

    </script>
</body>
</html>

