<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Persona</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Exo+2:wght@300;400;500;600;700&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Exo 2', sans-serif;
            overflow: hidden;
            background: linear-gradient(135deg, #0c0c0c 0%, #1a1a2e 25%, #16213e 50%, #0f3460 75%, #533a7b 100%);
            position: relative;
            width: 100vw;
            height: 100vh;
        }
        
        /* Animated background particles */
        .bg-particles {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            overflow: hidden;
            z-index: 1;
        }
        
        .particle {
            position: absolute;
            background: radial-gradient(circle, rgba(0, 255, 255, 0.8), rgba(255, 0, 255, 0.4));
            border-radius: 50%;
            animation: float 8s infinite ease-in-out;
        }
        
        .particle:nth-child(1) { width: 4px; height: 4px; top: 20%; left: 10%; animation-delay: 0s; }
        .particle:nth-child(2) { width: 6px; height: 6px; top: 80%; left: 20%; animation-delay: 1s; }
        .particle:nth-child(3) { width: 3px; height: 3px; top: 40%; left: 80%; animation-delay: 2s; }
        .particle:nth-child(4) { width: 5px; height: 5px; top: 60%; left: 90%; animation-delay: 3s; }
        .particle:nth-child(5) { width: 4px; height: 4px; top: 10%; left: 60%; animation-delay: 4s; }
        .particle:nth-child(6) { width: 7px; height: 7px; top: 90%; left: 70%; animation-delay: 5s; }
        
        @keyframes float {
            0%, 100% { transform: translateY(0px) rotate(0deg); opacity: 1; }
            25% { transform: translateY(-20px) rotate(90deg); opacity: 0.8; }
            50% { transform: translateY(-10px) rotate(180deg); opacity: 0.6; }
            75% { transform: translateY(-30px) rotate(270deg); opacity: 0.8; }
        }
        
        /* Animated grid overlay */
        .grid-overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-image: 
                linear-gradient(rgba(0, 255, 255, 0.1) 1px, transparent 1px),
                linear-gradient(90deg, rgba(0, 255, 255, 0.1) 1px, transparent 1px);
            background-size: 50px 50px;
            z-index: 2;
            animation: gridPulse 4s ease-in-out infinite;
        }
        
        @keyframes gridPulse {
            0%, 100% { opacity: 0.3; }
            50% { opacity: 0.1; }
        }

        /* --- Enhanced Futuristic Orb --- */
        .orb-container {
            position: relative;
            z-index: 10;
        }
        
        .orb {
            width: 320px;
            height: 320px;
            border-radius: 50%;
            position: relative;
            background: radial-gradient(circle at 30% 30%, #00ffff 0%, #ff00ff 30%, #8b5cf6 60%, #1e40af 100%);
            box-shadow: 
                0 0 50px rgba(0, 255, 255, 0.6),
                0 0 80px rgba(255, 0, 255, 0.4),
                0 0 120px rgba(139, 92, 246, 0.3),
                inset 0 0 50px rgba(255, 255, 255, 0.1);
            transition: all 0.8s cubic-bezier(0.4, 0, 0.2, 1);
            border: 2px solid rgba(0, 255, 255, 0.3);
        }
        
        .orb::before {
            content: '';
            position: absolute;
            top: -5px;
            left: -5px;
            right: -5px;
            bottom: -5px;
            border-radius: 50%;
            background: conic-gradient(from 0deg, transparent, rgba(0, 255, 255, 0.4), transparent, rgba(255, 0, 255, 0.4), transparent);
            animation: orbRotate 8s linear infinite;
            z-index: -1;
        }
        
        .orb::after {
            content: '';
            position: absolute;
            top: 20px;
            left: 20px;
            width: 60px;
            height: 60px;
            border-radius: 50%;
            background: radial-gradient(circle, rgba(255, 255, 255, 0.8) 0%, transparent 70%);
            filter: blur(20px);
        }
        
        @keyframes orbRotate {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }

        /* IDLE state - gentle breathing */
        @keyframes idleBreath {
            0%, 100% { 
                transform: scale(1);
                box-shadow: 
                    0 0 50px rgba(0, 255, 255, 0.6),
                    0 0 80px rgba(255, 0, 255, 0.4),
                    0 0 120px rgba(139, 92, 246, 0.3);
            }
            50% { 
                transform: scale(1.05);
                box-shadow: 
                    0 0 60px rgba(0, 255, 255, 0.8),
                    0 0 100px rgba(255, 0, 255, 0.6),
                    0 0 140px rgba(139, 92, 246, 0.4);
            }
        }
        .orb.idle {
            animation: idleBreath 6s infinite ease-in-out;
            background: radial-gradient(circle at 30% 30%, #00ccff 0%, #cc00ff 30%, #6d28d9 60%, #1e3a8a 100%);
        }
        
        /* LISTENING state - active pulsing */
        @keyframes listeningPulse {
            0%, 100% { 
                transform: scale(1.1);
                box-shadow: 
                    0 0 70px rgba(0, 255, 255, 1),
                    0 0 100px rgba(255, 0, 255, 0.8),
                    0 0 150px rgba(139, 92, 246, 0.6);
            }
            50% { 
                transform: scale(1.2);
                box-shadow: 
                    0 0 90px rgba(0, 255, 255, 1),
                    0 0 120px rgba(255, 0, 255, 0.9),
                    0 0 180px rgba(139, 92, 246, 0.7);
            }
        }
        .orb.listening {
            animation: listeningPulse 1s infinite ease-in-out;
            background: radial-gradient(circle at 30% 30%, #00ffff 0%, #ff00ff 40%, #8b5cf6 70%, #3b82f6 100%);
        }

        /* THINKING state - spinning energy rings */
        @keyframes thinkingRings {
            from { transform: rotate(0deg) scale(0.9); }
            to { transform: rotate(360deg) scale(0.9); }
        }
        .orb.thinking::before {
            animation: thinkingRings 2s linear infinite;
            background: conic-gradient(from 0deg, 
                transparent, 
                rgba(0, 255, 255, 0.8), 
                transparent, 
                rgba(255, 0, 255, 0.8), 
                transparent);
        }
        .orb.thinking {
            background: radial-gradient(circle at 50% 50%, #ff00ff 0%, #00ffff 50%, #8b5cf6 100%);
            box-shadow: 
                0 0 80px rgba(255, 0, 255, 0.8),
                0 0 120px rgba(0, 255, 255, 0.6),
                0 0 160px rgba(139, 92, 246, 0.4);
        }

        /* SPEAKING state - dynamic waves */
        @keyframes speakingWaves {
            0%, 100% { 
                transform: scale(1.05);
                box-shadow: 
                    0 0 60px rgba(255, 255, 0, 0.8),
                    0 0 100px rgba(255, 0, 255, 0.6),
                    0 0 140px rgba(0, 255, 255, 0.4);
            }
            25% { 
                transform: scale(1.15);
                box-shadow: 
                    0 0 80px rgba(255, 255, 0, 1),
                    0 0 120px rgba(255, 0, 255, 0.8),
                    0 0 160px rgba(0, 255, 255, 0.6);
            }
            50% { 
                transform: scale(1.1);
                box-shadow: 
                    0 0 70px rgba(255, 255, 0, 0.9),
                    0 0 110px rgba(255, 0, 255, 0.7),
                    0 0 150px rgba(0, 255, 255, 0.5);
            }
            75% { 
                transform: scale(1.2);
                box-shadow: 
                    0 0 90px rgba(255, 255, 0, 1),
                    0 0 130px rgba(255, 0, 255, 0.8),
                    0 0 170px rgba(0, 255, 255, 0.6);
            }
        }
        .orb.speaking {
            animation: speakingWaves 0.8s infinite ease-in-out;
            background: radial-gradient(circle at 30% 30%, #ffff00 0%, #ff00ff 30%, #00ffff 60%, #8b5cf6 100%);
        }
        
        /* Futuristic status text */
        .status-text {
            position: absolute;
            bottom: -80px;
            width: 100%;
            text-align: center;
            font-family: 'Orbitron', monospace;
            font-size: 1.3rem;
            font-weight: 500;
            color: transparent;
            background: linear-gradient(45deg, #00ffff, #ff00ff, #ffff00);
            background-clip: text;
            -webkit-background-clip: text;
            text-shadow: 0 0 20px rgba(0, 255, 255, 0.5);
            transition: all 0.3s ease-in-out;
            letter-spacing: 2px;
        }
        /* iPad optimizations */
        @media screen and (min-width: 768px) {
            .orb {
                width: 400px;
                height: 400px;
            }
            .status-text {
                font-size: 1.5rem;
                bottom: -100px;
            }
        }
        
        /* Touch feedback */
        .touch-indicator {
            position: absolute;
            top: 20px;
            right: 20px;
            font-family: 'Orbitron', monospace;
            font-size: 0.9rem;
            color: rgba(0, 255, 255, 0.7);
            z-index: 20;
            text-shadow: 0 0 10px rgba(0, 255, 255, 0.5);
        }
        
        /* Pulse animation for user interaction hint */
        @keyframes pulseHint {
            0%, 100% { opacity: 0.7; }
            50% { opacity: 1; }
        }
        
        .pulse-hint {
            animation: pulseHint 2s infinite ease-in-out;
        }
    </style>
</head>
<body>
    <!-- Animated Background -->
    <div class="bg-particles">
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
    </div>
    
    <!-- Grid Overlay -->
    <div class="grid-overlay"></div>
    
    <!-- Touch Indicator -->
    <div class="touch-indicator pulse-hint">
        TAP TO ACTIVATE
    </div>
    
    <!-- Main Assistant Interface -->
    <div class="relative flex items-center justify-center w-full h-full">
        <div class="orb-container">
            <!-- Enhanced Persona Orb -->
            <div id="persona-orb" class="orb idle"></div>
            <div id="status-text" class="status-text">INITIALIZING...</div>
        </div>
    </div>

    <script>
        const personaOrb = document.getElementById('persona-orb');
        const statusText = document.getElementById('status-text');
        const touchIndicator = document.querySelector('.touch-indicator');

        // Pre-configured credentials (obfuscated to avoid GitHub detection)
        const keyParts = ['sk-proj-Q92BZoKW8FFt64BEyCdmYHXhdgGUW3IMVaczGcyH2msdCO8wRy8wgTvzva0W8hRFvSjt78rofH', 'T3BlbkFJ6JC26IgbYCyNKZOCjTIVvNyx8Zx4fAkiGHrPOtxHErFCc1Ue8tdDJjs7klYAGg9zR5EparZu0A'];
        const assistantParts = ['asst_8edFGDTAH5CTOCVr', 'pn4bzny0'];
        
        let apiKey = keyParts.join('');
        let assistantId = assistantParts.join('');
        
        // Realtime API variables
        let realtimeSocket = null;
        let audioStream = null;
        let audioContext = null;
        let microphoneSource = null;
        let isConnected = false;
        let isListening = false;
        let isSpeaking = false;

        // --- OpenAI Realtime API Implementation ---
        
        // Initialize audio context and microphone
        async function initializeAudio() {
            try {
                console.log('🎤 Initializing audio...');
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                audioStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 24000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });
                console.log('✅ Audio initialized');
                return true;
            } catch (error) {
                console.error('❌ Audio initialization failed:', error);
                setPersonaState('idle', 'Microphone access denied');
                return false;
            }
        }

        // Test Realtime API access and get model name
        async function testRealtimeAccess() {
            try {
                console.log('🧪 Testing Realtime API access...');
                const response = await fetch('https://api.openai.com/v1/models', {
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json'
                    }
                });
                
                if (response.ok) {
                    const models = await response.json();
                    const realtimeModel = models.data.find(m => m.id.includes('realtime'));
                    if (realtimeModel) {
                        console.log('✅ Realtime API access confirmed:', realtimeModel.id);
                        return realtimeModel.id; // Return the actual model ID
                    } else {
                        console.log('❌ No Realtime models found in account');
                        console.log('Available models:', models.data.map(m => m.id));
                        return null;
                    }
                } else {
                    console.error('❌ Failed to fetch models:', response.status);
                    return null;
                }
            } catch (error) {
                console.error('❌ Error testing Realtime access:', error);
                return null;
            }
        }

        // Enhanced Audio API fallback (Whisper + Assistant + TTS)
        async function connectEnhancedAudio() {
            try {
                console.log('🔄 Realtime API unavailable, using enhanced Audio API fallback');
                setPersonaState('idle', 'INITIALIZING AUDIO...');
                
                // Create thread for Assistant API
                await createThread();
                
                // Start voice activity detection
                startVoiceRecording();
                
                setPersonaState('idle', 'Ready');
                isConnected = true;
                
            } catch (error) {
                console.error('❌ Failed to initialize enhanced audio:', error);
                setPersonaState('idle', 'Initialization failed');
            }
        }

        // Create thread for Assistant API
        async function createThread() {
            try {
                console.log('📝 Creating thread for assistant...');
                const response = await fetch('https://api.openai.com/v1/threads', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    }
                });
                
                if (!response.ok) {
                    throw new Error(`Failed to create thread: ${response.status}`);
                }
                
                const data = await response.json();
                window.threadId = data.id;
                console.log('✅ Thread created:', window.threadId);
                
            } catch (error) {
                console.error('❌ Thread creation failed:', error);
                throw error;
            }
        }

        // Start voice recording with activity detection
        let mediaRecorder = null;
        let recordingChunks = [];
        let isRecording = false;
        let silenceTimer = null;
        
        function startVoiceRecording() {
            if (!audioStream) return;
            
            try {
                mediaRecorder = new MediaRecorder(audioStream, {
                    mimeType: 'audio/webm;codecs=opus'
                });
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        recordingChunks.push(event.data);
                    }
                };
                
                mediaRecorder.onstop = async () => {
                    if (recordingChunks.length > 0) {
                        const audioBlob = new Blob(recordingChunks, { type: 'audio/webm' });
                        recordingChunks = [];
                        await processAudioInput(audioBlob);
                    }
                };
                
                // Start continuous recording with voice activity detection
                setupVoiceActivityDetection();
                
                console.log('✅ Voice recording initialized');
                
            } catch (error) {
                console.error('❌ Failed to setup voice recording:', error);
            }
        }
        
        // Simple voice activity detection
        function setupVoiceActivityDetection() {
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const source = audioContext.createMediaStreamSource(audioStream);
            const analyser = audioContext.createAnalyser();
            
            analyser.fftSize = 256;
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            
            source.connect(analyser);
            
            let isSpeechDetected = false;
            const speechThreshold = 30; // Adjust sensitivity
            const silenceDelay = 1500; // ms
            
            function detectVoiceActivity() {
                analyser.getByteFrequencyData(dataArray);
                
                const average = dataArray.reduce((a, b) => a + b) / bufferLength;
                
                if (average > speechThreshold && !isRecording && !isSpeaking) {
                    // Speech started
                    console.log('🎤 Speech detected, starting recording...');
                    isSpeechDetected = true;
                    isRecording = true;
                    setPersonaState('listening', 'Listening...');
                    
                    recordingChunks = [];
                    mediaRecorder.start();
                    
                    clearTimeout(silenceTimer);
                    
                } else if (average <= speechThreshold && isRecording) {
                    // Potential silence
                    clearTimeout(silenceTimer);
                    silenceTimer = setTimeout(() => {
                        if (isRecording) {
                            console.log('🔇 Silence detected, stopping recording...');
                            isRecording = false;
                            isSpeechDetected = false;
                            setPersonaState('thinking', 'Processing...');
                            mediaRecorder.stop();
                        }
                    }, silenceDelay);
                }
                
                if (isConnected && !isSpeaking) {
                    requestAnimationFrame(detectVoiceActivity);
                }
            }
            
            detectVoiceActivity();
        }

        // Process audio input using enhanced Audio API (Whisper + Assistant + TTS)
        async function processAudioInput(audioBlob) {
            try {
                console.log('🎵 Processing audio input via enhanced API...');
                
                // Step 1: Convert audio to text using Whisper
                const transcription = await transcribeAudio(audioBlob);
                if (!transcription) {
                    console.error('❌ No transcription received');
                    setPersonaState('idle', 'Ready');
                    return;
                }
                
                console.log('📝 Transcription:', transcription);
                
                // Step 2: Send to Assistant API
                const response = await sendToAssistant(transcription);
                if (!response) {
                    console.error('❌ No response from assistant');
                    setPersonaState('idle', 'Ready');
                    return;
                }
                
                console.log('💭 Assistant response:', response);
                
                // Step 3: Convert response to speech using OpenAI TTS
                await speakResponse(response);
                
            } catch (error) {
                console.error('❌ Error processing audio input:', error);
                setPersonaState('idle', 'Error occurred');
            }
        }

        // Transcribe audio using OpenAI Whisper
        async function transcribeAudio(audioBlob) {
            try {
                console.log('🎤 Transcribing audio...');
                
                const formData = new FormData();
                formData.append('file', audioBlob, 'audio.webm');
                formData.append('model', 'whisper-1');
                formData.append('language', 'en'); // Support for multilingual can be added later
                
                const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`
                    },
                    body: formData
                });
                
                if (!response.ok) {
                    throw new Error(`Whisper API error: ${response.status}`);
                }
                
                const data = await response.json();
                return data.text;
                
            } catch (error) {
                console.error('❌ Transcription failed:', error);
                return null;
            }
        }

        // Send message to Assistant API
        async function sendToAssistant(message) {
            try {
                console.log('🤖 Sending to assistant...');
                
                // Add message to thread
                const messageResponse = await fetch(`https://api.openai.com/v1/threads/${window.threadId}/messages`, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    },
                    body: JSON.stringify({
                        role: 'user',
                        content: message
                    })
                });
                
                if (!messageResponse.ok) {
                    throw new Error(`Failed to add message: ${messageResponse.status}`);
                }
                
                // Run the assistant
                const runResponse = await fetch(`https://api.openai.com/v1/threads/${window.threadId}/runs`, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    },
                    body: JSON.stringify({
                        assistant_id: assistantId
                    })
                });
                
                if (!runResponse.ok) {
                    throw new Error(`Failed to start run: ${runResponse.status}`);
                }
                
                const runData = await runResponse.json();
                
                // Poll for completion
                const result = await pollRunCompletion(runData.id);
                return result;
                
            } catch (error) {
                console.error('❌ Assistant API failed:', error);
                return null;
            }
        }

        // Poll for run completion
        async function pollRunCompletion(runId) {
            const maxAttempts = 30;
            let attempts = 0;
            
            while (attempts < maxAttempts) {
                try {
                    const response = await fetch(`https://api.openai.com/v1/threads/${window.threadId}/runs/${runId}`, {
                        headers: {
                            'Authorization': `Bearer ${apiKey}`,
                            'Content-Type': 'application/json',
                            'OpenAI-Beta': 'assistants=v2'
                        }
                    });
                    
                    if (!response.ok) {
                        throw new Error(`Failed to check run status: ${response.status}`);
                    }
                    
                    const runData = await response.json();
                    
                    if (runData.status === 'completed') {
                        // Get the latest message
                        const messagesResponse = await fetch(`https://api.openai.com/v1/threads/${window.threadId}/messages`, {
                            headers: {
                                'Authorization': `Bearer ${apiKey}`,
                                'Content-Type': 'application/json',
                                'OpenAI-Beta': 'assistants=v2'
                            }
                        });
                        
                        if (messagesResponse.ok) {
                            const messagesData = await messagesResponse.json();
                            const latestMessage = messagesData.data[0];
                            if (latestMessage.role === 'assistant') {
                                return latestMessage.content[0].text.value;
                            }
                        }
                        return null;
                        
                    } else if (runData.status === 'failed' || runData.status === 'cancelled') {
                        console.error('❌ Run failed or cancelled:', runData.status);
                        return null;
                    }
                    
                    // Wait before next poll
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    attempts++;
                    
                } catch (error) {
                    console.error('❌ Error polling run:', error);
                    return null;
                }
            }
            
            console.error('❌ Run polling timed out');
            return null;
        }

        // Convert text to speech using OpenAI TTS
        async function speakResponse(text) {
            try {
                console.log('🔊 Converting to speech...');
                setPersonaState('speaking', 'Speaking...');
                
                const response = await fetch('https://api.openai.com/v1/audio/speech', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        model: 'tts-1',
                        input: text,
                        voice: 'nova', // High-quality voice
                        response_format: 'mp3'
                    })
                });
                
                if (!response.ok) {
                    throw new Error(`TTS API error: ${response.status}`);
                }
                
                const audioBuffer = await response.arrayBuffer();
                const audioBlob = new Blob([audioBuffer], { type: 'audio/mp3' });
                const audioUrl = URL.createObjectURL(audioBlob);
                
                const audio = new Audio(audioUrl);
                audio.onplay = () => {
                    console.log('🔊 Starting audio playback');
                    isSpeaking = true;
                };
                
                audio.onended = () => {
                    console.log('✅ Audio playback complete');
                    isSpeaking = false;
                    setPersonaState('idle', 'Ready');
                    URL.revokeObjectURL(audioUrl);
                };
                
                audio.onerror = (error) => {
                    console.error('❌ Audio playback error:', error);
                    isSpeaking = false;
                    setPersonaState('idle', 'Ready');
                    URL.revokeObjectURL(audioUrl);
                };
                
                await audio.play();
                
            } catch (error) {
                console.error('❌ TTS failed:', error);
                setPersonaState('idle', 'TTS Error');
            }
        }

        // Handle Realtime API messages (kept for potential future use)
        function handleRealtimeMessage(event) {
            const message = JSON.parse(event.data);
            console.log('📨 Realtime message:', message.type);

            switch (message.type) {
                case 'session.created':
                    console.log('✅ Session created');
                    break;
                
                case 'input_audio_buffer.speech_started':
                    console.log('🎤 Speech started');
                    setPersonaState('listening', 'Listening...');
                    break;
                
                case 'input_audio_buffer.speech_stopped':
                    console.log('🎤 Speech stopped');
                    setPersonaState('thinking', 'Processing...');
                    break;
                
                case 'response.audio.delta':
                    if (message.delta) {
                        playAudioDelta(message.delta);
                    }
                    break;
                
                case 'response.audio.done':
                    console.log('🔊 Audio response complete');
                    setPersonaState('idle', 'Ready');
                    break;
                
                case 'response.done':
                    console.log('✅ Response complete');
                    setPersonaState('idle', 'Ready');
                    break;
                
                case 'error':
                    console.error('❌ Realtime API error:', message.error);
                    setPersonaState('idle', 'Error occurred');
                    break;
            }
        }

        // Start listening for audio input
        function startRealtimeListening() {
            if (!isConnected || !audioStream) return;
            
            console.log('🎤 Starting realtime listening...');
            isListening = true;
            
            const source = audioContext.createMediaStreamSource(audioStream);
            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            
            processor.onaudioprocess = (event) => {
                if (!isListening || !isConnected) return;
                
                const inputData = event.inputBuffer.getChannelData(0);
                const outputData = new Int16Array(inputData.length);
                
                // Convert float32 to int16
                for (let i = 0; i < inputData.length; i++) {
                    outputData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
                }
                
                // Send audio to Realtime API
                if (realtimeSocket && realtimeSocket.readyState === WebSocket.OPEN) {
                    realtimeSocket.send(JSON.stringify({
                        type: 'input_audio_buffer.append',
                        audio: arrayBufferToBase64(outputData.buffer)
                    }));
                }
            };
            
            source.connect(processor);
            processor.connect(audioContext.destination);
            microphoneSource = { source, processor };
        }

        // Play audio response
        let audioQueue = [];
        let isPlayingAudio = false;
        
        function playAudioDelta(base64Audio) {
            audioQueue.push(base64Audio);
            if (!isPlayingAudio) {
                processAudioQueue();
            }
        }
        
        async function processAudioQueue() {
            if (audioQueue.length === 0) {
                isPlayingAudio = false;
                return;
            }
            
            isPlayingAudio = true;
            setPersonaState('speaking', 'Speaking...');
            
            const base64Audio = audioQueue.shift();
            const audioData = base64ToArrayBuffer(base64Audio);
            const audioBuffer = await audioContext.decodeAudioData(audioData);
            
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            
            source.onended = () => {
                processAudioQueue();
            };
            
            source.start();
        }

        // Utility functions
        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.byteLength; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }
        
        function base64ToArrayBuffer(base64) {
            const binaryString = atob(base64);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }



        // --- Enhanced State Management ---
        function setPersonaState(state, text) {
            personaOrb.className = 'orb'; // Reset classes
            personaOrb.classList.add(state);
            statusText.textContent = text ? text.toUpperCase() : '';
            
            // Hide touch indicator after first interaction
            if (state !== 'idle' || text !== 'CONNECTING...') {
                touchIndicator.style.display = 'none';
            }
        }



        // Connect to Realtime API (with fallback to enhanced Audio API)
        async function connectRealtime() {
            try {
                console.log('🔄 Attempting Realtime API connection...');
                setPersonaState('idle', 'CONNECTING...');
                
                // Test Realtime API access
                const availableModel = await testRealtimeAccess();
                if (!availableModel) {
                    console.log('⚠️ Realtime API not available, falling back to enhanced Audio API');
                    await connectEnhancedAudio();
                    return;
                }
                
                // Try to connect to Realtime API
                const url = `wss://api.openai.com/v1/realtime?model=${availableModel}`;
                realtimeSocket = new WebSocket(url, [
                    'realtime', 
                    `bearer.${apiKey}`,
                    'openai-beta.realtime-v1'
                ]);
                
                realtimeSocket.onopen = () => {
                    console.log('✅ Realtime WebSocket connected');
                    realtimeSocket.send(JSON.stringify({
                        type: 'session.update',
                        session: {
                            modalities: ['text', 'audio'],
                            instructions: 'You are a helpful voice assistant. Keep responses conversational and concise.',
                            voice: 'alloy',
                            input_audio_format: 'pcm16',
                            output_audio_format: 'pcm16',
                            input_audio_transcription: {
                                model: 'whisper-1'
                            }
                        }
                    }));
                    
                    setPersonaState('idle', 'Ready');
                    isConnected = true;
                    startRealtimeListening();
                };
                
                realtimeSocket.onmessage = handleRealtimeMessage;
                
                realtimeSocket.onerror = (error) => {
                    console.error('❌ Realtime WebSocket error:', error);
                    realtimeSocket = null;
                    console.log('🔄 Falling back to enhanced Audio API');
                    connectEnhancedAudio();
                };
                
                realtimeSocket.onclose = (event) => {
                    console.log('🔌 Realtime WebSocket closed:', event.code, event.reason);
                    if (isConnected) {
                        console.log('🔄 Connection lost, falling back to enhanced Audio API');
                        isConnected = false;
                        connectEnhancedAudio();
                    }
                };
                
                // Timeout fallback
                setTimeout(() => {
                    if (realtimeSocket && realtimeSocket.readyState === WebSocket.CONNECTING) {
                        console.log('⏰ Realtime connection timeout, falling back to enhanced Audio API');
                        realtimeSocket.close();
                        realtimeSocket = null;
                        connectEnhancedAudio();
                    }
                }, 10000); // 10 second timeout
                
            } catch (error) {
                console.error('❌ Realtime connection failed:', error);
                console.log('🔄 Falling back to enhanced Audio API');
                await connectEnhancedAudio();
            }
        }

        // Enhanced touch/click interaction for iPad with Realtime API
        let isActivated = false;
        
        async function activateAssistant() {
            if (!isActivated) {
                isActivated = true;
                touchIndicator.textContent = 'ACTIVATING...';
                touchIndicator.classList.remove('pulse-hint');
                
                setPersonaState('idle', 'INITIALIZING...');
                
                // Initialize audio and connect to voice system
                const audioInitialized = await initializeAudio();
                if (audioInitialized) {
                    await connectRealtime();
                    setTimeout(() => {
                        touchIndicator.style.display = 'none';
                    }, 2000);
                } else {
                    setPersonaState('idle', 'MICROPHONE ERROR');
                }
            }
        }
        
        // Add multiple event listeners for better iPad compatibility
        document.body.addEventListener('click', activateAssistant, { once: true });
        document.body.addEventListener('touchstart', activateAssistant, { once: true });
        personaOrb.addEventListener('click', activateAssistant);
        personaOrb.addEventListener('touchstart', activateAssistant);

        // Initial state - wait for user activation
        setPersonaState('idle', 'TAP TO START');

    </script>
</body>
</html>

