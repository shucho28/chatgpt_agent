<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Persona</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap');
        
        body {
            font-family: 'Inter', sans-serif;
            overflow: hidden; /* Prevent scrolling */
        }

        /* --- Orb Styles & Animations --- */
        .orb {
            width: 250px; /* Made the orb a bit bigger */
            height: 250px;
            border-radius: 50%;
            position: relative;
            background: radial-gradient(circle, #5a67d8 0%, #3c366b 100%);
            box-shadow: 0 0 20px #5a67d8, 0 0 40px #5a67d8, 0 0 60px #c4b5fd;
            transition: all 0.5s ease-in-out;
        }

        /* IDLE state animation */
        @keyframes idlePulse {
            0%, 100% { transform: scale(1); box-shadow: 0 0 20px #5a67d8, 0 0 40px #5a67d8, 0 0 60px #c4b5fd; }
            50% { transform: scale(1.05); box-shadow: 0 0 25px #6a7ae9, 0 0 50px #6a7ae9, 0 0 75px #d8cffc; }
        }
        .orb.idle {
            animation: idlePulse 4s infinite ease-in-out;
            background: radial-gradient(circle, #4a55b8 0%, #2c265b 100%);
        }
        
        /* LISTENING state */
        .orb.listening {
            transform: scale(1.1);
            background: radial-gradient(circle, #6a7ae9 0%, #4c468b 100%);
        }

        /* THINKING state animation */
        @keyframes thinkingSpin {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }
        .orb.thinking::before {
            content: '';
            position: absolute;
            top: -10px;
            left: -10px;
            width: 270px;
            height: 270px;
            border-radius: 50%;
            border: 4px solid transparent;
            border-top-color: #a78bfa;
            border-bottom-color: #a78bfa;
            animation: thinkingSpin 1.5s linear infinite;
        }
        .orb.thinking {
            transform: scale(0.95);
        }

        /* SPEAKING state animation */
        @keyframes speakingGlow {
            0%, 100% { box-shadow: 0 0 20px #8b5cf6, 0 0 40px #8b5cf6, 0 0 60px #ddd6fe; transform: scale(1.02); }
            50% { box-shadow: 0 0 30px #a78bfa, 0 0 60px #a78bfa, 0 0 90px #ede9fe; transform: scale(1.05); }
        }
        .orb.speaking {
            animation: speakingGlow 1.2s infinite ease-in-out;
        }
        .status-text {
            position: absolute;
            bottom: -50px;
            width: 100%;
            text-align: center;
            font-size: 1.1rem;
            color: #a0aec0;
            transition: opacity 0.3s ease-in-out;
        }
    </style>
</head>
<body class="bg-gray-900 text-white flex flex-col items-center justify-center h-screen">

    
    <div class="relative flex items-center justify-center">
        <!-- Persona Orb -->
        <div id="persona-orb" class="orb idle"></div>
        <div id="status-text" class="status-text">Initializing...</div>
    </div>

    <script>
        const personaOrb = document.getElementById('persona-orb');
        const statusText = document.getElementById('status-text');

        // Pre-configured credentials (obfuscated to avoid GitHub detection)
        const keyParts = ['sk-proj-Q92BZoKW8FFt64BEyCdmYHXhdgGUW3IMVaczGcyH2msdCO8wRy8wgTvzva0W8hRFvSjt78rofH', 'T3BlbkFJ6JC26IgbYCyNKZOCjTIVvNyx8Zx4fAkiGHrPOtxHErFCc1Ue8tdDJjs7klYAGg9zR5EparZu0A'];
        const assistantParts = ['asst_8edFGDTAH5CTOCVr', 'pn4bzny0'];
        
        let apiKey = keyParts.join('');
        let assistantId = assistantParts.join('');
        let threadId = null;
        let isSpeaking = false;
        let isRecognitionActive = false; // Flag to prevent recognition restart errors

        // --- Speech Recognition & Synthesis Setup ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;

        function startListening() {
            if (isSpeaking || isRecognitionActive || !recognition) return;
            try {
                recognition.start();
            } catch (e) {
                // This can happen if start() is called while it's already starting.
                // The onend handler will manage the restart, so we can ignore this.
                console.info("Recognition could not be started, likely already starting. State will be managed by onend.", e);
                isRecognitionActive = false;
            }
        }

        function stopListening() {
            if (!recognition || !isRecognitionActive) return;
            recognition.stop();
        }

        function initializeSpeechRecognition() {
            if (SpeechRecognition) {
                recognition = new SpeechRecognition();
                recognition.continuous = false; 
                // Auto-detect languages - browser will handle multilingual recognition
                recognition.lang = 'en-US'; // Default, but browser auto-detects
                recognition.interimResults = false;
                recognition.maxAlternatives = 3; // Get multiple alternatives for better accuracy

                recognition.onstart = () => {
                    isRecognitionActive = true;
                    setPersonaState('listening', 'Listening...');
                };

                recognition.onend = () => {
                    isRecognitionActive = false;
                    // This is the single, reliable place to control the listening loop.
                    // It fires after a successful result, a no-speech timeout, or an error.
                    if (!isSpeaking && !personaOrb.classList.contains('thinking')) {
                        setPersonaState('idle', 'Ready');
                        setTimeout(() => startListening(), 100); 
                    }
                };
                
                recognition.onspeechstart = () => {
                     setPersonaState('listening', 'I hear you...');
                };

                recognition.onerror = (event) => {
                    // The 'no-speech' error is a normal timeout and not a cause for concern.
                    // We will not log it to the console. The 'onend' event handles the restart gracefully.
                    if (event.error !== 'no-speech') {
                        console.error('Speech recognition error:', event.error);
                    }
                    if (event.error === 'not-allowed') {
                         setPersonaState('idle', 'Microphone access denied.');
                         recognition = null; // Permanently disable if not allowed.
                    }
                };

                recognition.onresult = (event) => {
                    // Get the best result from alternatives
                    let bestTranscript = '';
                    let bestConfidence = 0;
                    
                    for (let i = 0; i < event.results[0].length; i++) {
                        const result = event.results[0][i];
                        if (result.confidence > bestConfidence) {
                            bestConfidence = result.confidence;
                            bestTranscript = result.transcript;
                        }
                    }
                    
                    const transcript = (bestTranscript || event.results[0][0].transcript).trim();
                    
                    if (transcript) {
                        console.log(`Recognized (${(bestConfidence * 100).toFixed(1)}%):`, transcript);
                        handleUserMessage(transcript);
                    }
                    // No need to call stopListening() here, as `onend` will fire automatically.
                };

            } else {
                setPersonaState('idle', "Voice input not supported.")
            }
        }

        // Initialize speech recognition and load voices on page load
        initializeSpeechRecognition();
        
        // Pre-load voices for faster TTS
        loadVoices().then(() => {
            console.log('Voices loaded:', voicesCache.length);
        });

        // --- New TTS Functions ---
        function base64ToArrayBuffer(base64) {
            const binaryString = window.atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }

        function pcmToWav(pcmData, sampleRate) {
            const numChannels = 1;
            const bitsPerSample = 16;
            const byteRate = sampleRate * numChannels * bitsPerSample / 8;
            const blockAlign = numChannels * bitsPerSample / 8;
            const dataSize = pcmData.byteLength;
            const chunkSize = 36 + dataSize;
            
            const buffer = new ArrayBuffer(44 + dataSize);
            const view = new DataView(buffer);

            // RIFF header
            view.setUint32(0, 0x52494646, false); // "RIFF"
            view.setUint32(4, chunkSize, true);
            view.setUint32(8, 0x57415645, false); // "WAVE"
            // "fmt " sub-chunk
            view.setUint32(12, 0x666d7420, false); // "fmt "
            view.setUint32(16, 16, true); // Sub-chunk size
            view.setUint16(20, 1, true); // Audio format (1 for PCM)
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, byteRate, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, bitsPerSample, true);
            // "data" sub-chunk
            view.setUint32(36, 0x64617461, false); // "data"
            view.setUint32(40, dataSize, true);

            const pcmAsInt16 = new Int16Array(pcmData);
            const data = new Int16Array(buffer, 44);
            data.set(pcmAsInt16);

            return new Blob([view], { type: 'audio/wav' });
        }


        // Language detection function
        function detectLanguage(text) {
            // Simple language detection based on character patterns
            const hindiPattern = /[\u0900-\u097F]/;
            const marathiPattern = /[\u0900-\u097F]/; // Marathi uses same Devanagari script as Hindi
            const angikaPattern = /[\u0900-\u097F]/; // Angika also uses Devanagari
            const frenchPattern = /[àâäéèêëïîôùûüÿç]|(\b(le|la|les|de|du|des|un|une|et|est|dans|pour|avec|sur|par|ce|cette|son|sa|ses|mon|ma|mes|ton|ta|tes|notre|nos|votre|vos|leur|leurs)\b)/i;
            
            if (hindiPattern.test(text)) {
                // Could be Hindi, Marathi, or Angika - use context clues
                if (/(मराठी|महाराष्ट्र|पुणे|मुंबई)/i.test(text)) return 'mr-IN';
                if (/(अंगिका|अंग)/i.test(text)) return 'hi-IN'; // Fallback to Hindi for Angika
                return 'hi-IN';
            }
            if (frenchPattern.test(text)) return 'fr-FR';
            return 'en-US'; // Default to English
        }

        // Cache for loaded voices
        let voicesCache = [];
        let voicesLoaded = false;
        
        // Pre-load and cache voices
        function loadVoices() {
            return new Promise((resolve) => {
                const voices = speechSynthesis.getVoices();
                if (voices.length > 0) {
                    voicesCache = voices;
                    voicesLoaded = true;
                    resolve(voices);
                } else {
                    speechSynthesis.addEventListener('voiceschanged', () => {
                        voicesCache = speechSynthesis.getVoices();
                        voicesLoaded = true;
                        resolve(voicesCache);
                    }, { once: true });
                }
            });
        }

        // Get best quality voice for language
        function getBestVoice(language) {
            const voices = voicesLoaded ? voicesCache : speechSynthesis.getVoices();
            
            // Enhanced voice preferences prioritizing natural-sounding voices
            const voicePreferences = {
                'en-US': [
                    // Neural/Premium voices first
                    'Google US English', 'Microsoft Aria Online', 'Microsoft Jenny Online',
                    'Samantha', 'Alex', 'Victoria', 'Karen', 'Moira',
                    // Fallback to any US English
                    'Microsoft Zira', 'Microsoft David'
                ],
                'hi-IN': [
                    'Google हिन्दी', 'Microsoft Hemant Online', 'Microsoft Kalpana Online',
                    'Lekha', 'Google Hindi', 'Microsoft Hemant'
                ],
                'fr-FR': [
                    'Google français', 'Microsoft Denise Online', 'Microsoft Eloise Online',
                    'Thomas', 'Amelie', 'Microsoft Hortense'
                ],
                'mr-IN': [
                    'Google हिन्दी', 'Microsoft Hemant Online', 'Microsoft Hemant'
                ]
            };
            
            const preferences = voicePreferences[language] || voicePreferences['en-US'];
            
            // First pass: exact name matches (premium voices)
            for (const prefName of preferences) {
                const voice = voices.find(v => v.name === prefName);
                if (voice) return voice;
            }
            
            // Second pass: partial matches
            for (const prefName of preferences) {
                const voice = voices.find(v => v.name.includes(prefName.split(' ')[0]));
                if (voice) return voice;
            }
            
            // Third pass: language match with quality filter
            const langVoices = voices.filter(v => v.lang === language);
            if (langVoices.length > 0) {
                // Prefer voices with "Google" or "Microsoft" for better quality
                const qualityVoice = langVoices.find(v => 
                    v.name.includes('Google') || v.name.includes('Microsoft')
                );
                if (qualityVoice) return qualityVoice;
                return langVoices[0];
            }
            
            // Final fallback to best English voice
            const englishVoice = voices.find(v => 
                v.lang.startsWith('en') && (v.name.includes('Google') || v.name.includes('Samantha'))
            );
            return englishVoice || voices[0];
        }

        async function speak(text) {
            if (!text) {
                 isSpeaking = false; // Ensure state is correct
                 setPersonaState('idle', 'Ready');
                 startListening();
                 return;
            }
            isSpeaking = true;
            stopListening(); // Explicitly stop before speaking
            setPersonaState('speaking', '...');
            
            try {
                // Ensure voices are loaded first (reduces latency)
                if (!voicesLoaded) {
                    await loadVoices();
                }
                
                // Try using browser's built-in speech synthesis
                if ('speechSynthesis' in window) {
                    // Cancel any ongoing speech to reduce latency
                    speechSynthesis.cancel();
                    
                    const utterance = new SpeechSynthesisUtterance();
                    
                    // Detect language and set appropriate voice
                    const detectedLang = detectLanguage(text);
                    const selectedVoice = getBestVoice(detectedLang);
                    
                    if (selectedVoice) {
                        utterance.voice = selectedVoice;
                        utterance.lang = selectedVoice.lang;
                    } else {
                        utterance.lang = detectedLang;
                    }
                    
                    // Optimized settings for natural, human-like speech
                    utterance.rate = 0.95; // Slightly faster, more natural
                    utterance.pitch = 0.95; // Slightly lower pitch for warmth
                    utterance.volume = 1.0;
                    
                    // Minimal text processing to reduce robotic feeling
                    const processedText = text
                        .replace(/\s+/g, ' ') // Clean up multiple spaces
                        .trim();
                    
                    utterance.text = processedText;
                    
                    // Promise-based approach for better error handling
                    const speakPromise = new Promise((resolve, reject) => {
                        utterance.onend = () => {
                            isSpeaking = false;
                            setPersonaState('idle', 'Ready');
                            startListening();
                            resolve();
                        };
                        
                        utterance.onerror = (event) => {
                            console.error('Speech synthesis error:', event.error);
                            isSpeaking = false;
                            setPersonaState('idle', 'Speech Error');
                            startListening();
                            reject(event.error);
                        };
                        
                        // Immediate speech synthesis for reduced latency
                        speechSynthesis.speak(utterance);
                    });
                    
                    return speakPromise;
                }
                
                // Fallback: If no TTS available, just continue
                console.warn('No TTS available');
                isSpeaking = false;
                setPersonaState('idle', 'Ready');
                startListening();
                return;
                
                // Original Gemini TTS code (commented out - requires API key)
                /*
                const geminiApiKey = "YOUR_GEMINI_API_KEY_HERE"; 
                const ttsApiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key=${geminiApiKey}`;
                const payload = {
                    contents: [{ parts: [{ text: text }] }],
                    generationConfig: {
                        responseModalities: ["AUDIO"],
                        speechConfig: {
                            voiceConfig: { prebuiltVoiceConfig: { voiceName: "Puck" } } // A pleasant, upbeat voice
                        }
                    },
                    model: "gemini-2.5-flash-preview-tts"
                };

                const response = await fetch(ttsApiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                if (!response.ok) {
                    const error = await response.json();
                    console.error('TTS API Error:', error);
                    throw new Error('Failed to generate audio.');
                }

                const result = await response.json();
                const part = result?.candidates?.[0]?.content?.parts?.[0];
                const audioData = part?.inlineData?.data;
                const mimeType = part?.inlineData?.mimeType;

                if (audioData && mimeType && mimeType.startsWith("audio/")) {
                    const sampleRateMatch = mimeType.match(/rate=(\d+)/);
                    const sampleRate = sampleRateMatch ? parseInt(sampleRateMatch[1], 10) : 24000;
                    const pcmData = base64ToArrayBuffer(audioData);
                    const wavBlob = pcmToWav(pcmData, sampleRate);
                    const audioUrl = URL.createObjectURL(wavBlob);
                    
                    const audio = new Audio(audioUrl);
                    audio.onended = () => {
                        isSpeaking = false;
                        setPersonaState('idle', 'Ready');
                        URL.revokeObjectURL(audioUrl);
                        startListening(); // Kickstart the listening loop
                    };
                    audio.onerror = (e) => {
                         console.error('Audio playback error', e);
                         isSpeaking = false;
                         setPersonaState('idle', 'Audio Error');
                         startListening(); // Attempt to recover
                    };
                    audio.play();
                } else {
                     throw new Error('No audio data in response.');
                }
                */

            } catch(e) {
                 console.error('Error in speak function:', e);
                 isSpeaking = false;
                 setPersonaState('idle', 'TTS Failed');
                 startListening(); // Attempt to recover
            }
        }


        // --- State Management ---
        function setPersonaState(state, text) {
            personaOrb.className = 'orb'; // Reset classes
            personaOrb.classList.add(state);
            statusText.textContent = text || '';
        }

        // --- OpenAI Assistant API Interaction ---
        
        async function createThread() {
            try {
                const response = await fetch('https://api.openai.com/v1/threads', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    }
                });
                if (!response.ok) {
                    const errorData = await response.json().catch(() => ({ error: { message: 'Unknown API error' } }));
                    const errorMsg = errorData.error?.message || 'Failed to create thread';
                    if (response.status === 401) {
                        setPersonaState('idle', 'Invalid API key');
                    } else if (response.status === 429) {
                        setPersonaState('idle', 'Rate limited');
                    } else {
                        setPersonaState('idle', 'Connection failed');
                    }
                    throw new Error(`API Error: ${errorMsg}`);
                }
                const data = await response.json();
                threadId = data.id;
                console.log("Thread created:", threadId);
                setPersonaState('idle', "Ready");
                startListening();
            } catch (error) {
                console.error('Thread creation error:', error);
                if (!personaOrb.classList.contains('idle')) {
                    setPersonaState('idle', 'Connection failed');
                }
            }
        }

        async function handleUserMessage(prompt) {
            if (!prompt) return;
            
            // Recognition has already stopped via the onresult event.
            setPersonaState('thinking', 'Thinking...');

            if (!threadId) {
                await createThread();
                if(!threadId) {
                    await speak("I couldn't start a conversation. Please check the API keys and refresh.");
                    return;
                }
            }
            
            try {
                // 1. Add message to thread
                const messageResponse = await fetch(`https://api.openai.com/v1/threads/${threadId}/messages`, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    },
                    body: JSON.stringify({
                        role: 'user',
                        content: prompt
                    })
                });
                
                if (!messageResponse.ok) {
                    throw new Error(`Failed to add message: ${messageResponse.status}`);
                }

                // 2. Create a run
                const runResponse = await fetch(`https://api.openai.com/v1/threads/${threadId}/runs`, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    },
                    body: JSON.stringify({
                        assistant_id: assistantId
                    })
                });
                
                if (!runResponse.ok) {
                    const errorData = await runResponse.json().catch(() => ({}));
                    const errorMsg = errorData.error?.message || 'Failed to create run';
                    throw new Error(errorMsg);
                }
                
                const run = await runResponse.json();

                // 3. Poll for run completion with timeout
                let runStatus;
                let pollCount = 0;
                const maxPolls = 30; // 30 seconds timeout
                
                do {
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    const statusResponse = await fetch(`https://api.openai.com/v1/threads/${threadId}/runs/${run.id}`, {
                        headers: {
                            'Authorization': `Bearer ${apiKey}`,
                            'OpenAI-Beta': 'assistants=v2'
                        }
                    });
                    
                    if (!statusResponse.ok) {
                        throw new Error('Failed to check run status');
                    }
                    
                    const statusData = await statusResponse.json();
                    runStatus = statusData.status;
                    pollCount++;
                    
                    if (pollCount >= maxPolls) {
                        throw new Error('Request timeout - please try again');
                    }
                } while (runStatus === 'in_progress' || runStatus === 'queued');

                if(runStatus !== 'completed') {
                    throw new Error(`Assistant error: ${runStatus}`);
                }

                // 4. Retrieve latest messages
                const messagesResponse = await fetch(`https://api.openai.com/v1/threads/${threadId}/messages`, {
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'OpenAI-Beta': 'assistants=v2'
                    }
                });
                
                if (!messagesResponse.ok) {
                    throw new Error('Failed to retrieve messages');
                }
                
                const messagesData = await messagesResponse.json();
                const assistantMessage = messagesData.data[0]?.content[0]?.text?.value;
                
                if (!assistantMessage) {
                    throw new Error('No response from assistant');
                }
                
                await speak(assistantMessage);

            } catch (error) {
                console.error("Error during assistant interaction:", error);
                let errorMessage = "I'm having trouble right now. ";
                
                if (error.message.includes('timeout')) {
                    errorMessage += "The request took too long. Please try again.";
                } else if (error.message.includes('401')) {
                    errorMessage += "Please check your API credentials.";
                } else if (error.message.includes('429')) {
                    errorMessage += "Too many requests. Please wait a moment.";
                } else {
                    errorMessage += "Please try again in a moment.";
                }
                
                await speak(errorMessage);
            }
        }


        // Add a click listener to the body to start on user interaction
        document.body.addEventListener('click', () => {
            if(recognition && !isSpeaking && !isRecognitionActive) {
                startListening();
            }
        }, { once: true });


        // Auto-connect immediately on page load
        setPersonaState('idle', 'Connecting...');
        createThread();

    </script>
</body>
</html>

