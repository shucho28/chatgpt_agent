<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Persona</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Exo+2:wght@300;400;500;600;700&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Exo 2', sans-serif;
            overflow: hidden;
            background: linear-gradient(135deg, #0c0c0c 0%, #1a1a2e 25%, #16213e 50%, #0f3460 75%, #533a7b 100%);
            position: relative;
            width: 100vw;
            height: 100vh;
        }
        
        /* Animated background particles */
        .bg-particles {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            overflow: hidden;
            z-index: 1;
        }
        
        .particle {
            position: absolute;
            background: radial-gradient(circle, rgba(0, 255, 255, 0.8), rgba(255, 0, 255, 0.4));
            border-radius: 50%;
            animation: float 8s infinite ease-in-out;
        }
        
        .particle:nth-child(1) { width: 4px; height: 4px; top: 20%; left: 10%; animation-delay: 0s; }
        .particle:nth-child(2) { width: 6px; height: 6px; top: 80%; left: 20%; animation-delay: 1s; }
        .particle:nth-child(3) { width: 3px; height: 3px; top: 40%; left: 80%; animation-delay: 2s; }
        .particle:nth-child(4) { width: 5px; height: 5px; top: 60%; left: 90%; animation-delay: 3s; }
        .particle:nth-child(5) { width: 4px; height: 4px; top: 10%; left: 60%; animation-delay: 4s; }
        .particle:nth-child(6) { width: 7px; height: 7px; top: 90%; left: 70%; animation-delay: 5s; }
        
        @keyframes float {
            0%, 100% { transform: translateY(0px) rotate(0deg); opacity: 1; }
            25% { transform: translateY(-20px) rotate(90deg); opacity: 0.8; }
            50% { transform: translateY(-10px) rotate(180deg); opacity: 0.6; }
            75% { transform: translateY(-30px) rotate(270deg); opacity: 0.8; }
        }
        
        /* Animated grid overlay */
        .grid-overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-image: 
                linear-gradient(rgba(0, 255, 255, 0.1) 1px, transparent 1px),
                linear-gradient(90deg, rgba(0, 255, 255, 0.1) 1px, transparent 1px);
            background-size: 50px 50px;
            z-index: 2;
            animation: gridPulse 4s ease-in-out infinite;
        }
        
        @keyframes gridPulse {
            0%, 100% { opacity: 0.3; }
            50% { opacity: 0.1; }
        }

        /* --- Enhanced Futuristic Orb --- */
        .orb-container {
            position: relative;
            z-index: 10;
        }
        
        .orb {
            width: 320px;
            height: 320px;
            border-radius: 50%;
            position: relative;
            background: radial-gradient(circle at 30% 30%, #00ffff 0%, #ff00ff 30%, #8b5cf6 60%, #1e40af 100%);
            box-shadow: 
                0 0 50px rgba(0, 255, 255, 0.6),
                0 0 80px rgba(255, 0, 255, 0.4),
                0 0 120px rgba(139, 92, 246, 0.3),
                inset 0 0 50px rgba(255, 255, 255, 0.1);
            transition: all 0.8s cubic-bezier(0.4, 0, 0.2, 1);
            border: 2px solid rgba(0, 255, 255, 0.3);
        }
        
        .orb::before {
            content: '';
            position: absolute;
            top: -5px;
            left: -5px;
            right: -5px;
            bottom: -5px;
            border-radius: 50%;
            background: conic-gradient(from 0deg, transparent, rgba(0, 255, 255, 0.4), transparent, rgba(255, 0, 255, 0.4), transparent);
            animation: orbRotate 8s linear infinite;
            z-index: -1;
        }
        
        .orb::after {
            content: '';
            position: absolute;
            top: 20px;
            left: 20px;
            width: 60px;
            height: 60px;
            border-radius: 50%;
            background: radial-gradient(circle, rgba(255, 255, 255, 0.8) 0%, transparent 70%);
            filter: blur(20px);
        }
        
        @keyframes orbRotate {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }

        /* IDLE state - gentle breathing */
        @keyframes idleBreath {
            0%, 100% { 
                transform: scale(1);
                box-shadow: 
                    0 0 50px rgba(0, 255, 255, 0.6),
                    0 0 80px rgba(255, 0, 255, 0.4),
                    0 0 120px rgba(139, 92, 246, 0.3);
            }
            50% { 
                transform: scale(1.05);
                box-shadow: 
                    0 0 60px rgba(0, 255, 255, 0.8),
                    0 0 100px rgba(255, 0, 255, 0.6),
                    0 0 140px rgba(139, 92, 246, 0.4);
            }
        }
        .orb.idle {
            animation: idleBreath 6s infinite ease-in-out;
            background: radial-gradient(circle at 30% 30%, #00ccff 0%, #cc00ff 30%, #6d28d9 60%, #1e3a8a 100%);
        }
        
        /* LISTENING state - active pulsing */
        @keyframes listeningPulse {
            0%, 100% { 
                transform: scale(1.1);
                box-shadow: 
                    0 0 70px rgba(0, 255, 255, 1),
                    0 0 100px rgba(255, 0, 255, 0.8),
                    0 0 150px rgba(139, 92, 246, 0.6);
            }
            50% { 
                transform: scale(1.2);
                box-shadow: 
                    0 0 90px rgba(0, 255, 255, 1),
                    0 0 120px rgba(255, 0, 255, 0.9),
                    0 0 180px rgba(139, 92, 246, 0.7);
            }
        }
        .orb.listening {
            animation: listeningPulse 1s infinite ease-in-out;
            background: radial-gradient(circle at 30% 30%, #00ffff 0%, #ff00ff 40%, #8b5cf6 70%, #3b82f6 100%);
        }

        /* THINKING state - spinning energy rings */
        @keyframes thinkingRings {
            from { transform: rotate(0deg) scale(0.9); }
            to { transform: rotate(360deg) scale(0.9); }
        }
        .orb.thinking::before {
            animation: thinkingRings 2s linear infinite;
            background: conic-gradient(from 0deg, 
                transparent, 
                rgba(0, 255, 255, 0.8), 
                transparent, 
                rgba(255, 0, 255, 0.8), 
                transparent);
        }
        .orb.thinking {
            background: radial-gradient(circle at 50% 50%, #ff00ff 0%, #00ffff 50%, #8b5cf6 100%);
            box-shadow: 
                0 0 80px rgba(255, 0, 255, 0.8),
                0 0 120px rgba(0, 255, 255, 0.6),
                0 0 160px rgba(139, 92, 246, 0.4);
        }

        /* SPEAKING state - dynamic waves */
        @keyframes speakingWaves {
            0%, 100% { 
                transform: scale(1.05);
                box-shadow: 
                    0 0 60px rgba(255, 255, 0, 0.8),
                    0 0 100px rgba(255, 0, 255, 0.6),
                    0 0 140px rgba(0, 255, 255, 0.4);
            }
            25% { 
                transform: scale(1.15);
                box-shadow: 
                    0 0 80px rgba(255, 255, 0, 1),
                    0 0 120px rgba(255, 0, 255, 0.8),
                    0 0 160px rgba(0, 255, 255, 0.6);
            }
            50% { 
                transform: scale(1.1);
                box-shadow: 
                    0 0 70px rgba(255, 255, 0, 0.9),
                    0 0 110px rgba(255, 0, 255, 0.7),
                    0 0 150px rgba(0, 255, 255, 0.5);
            }
            75% { 
                transform: scale(1.2);
                box-shadow: 
                    0 0 90px rgba(255, 255, 0, 1),
                    0 0 130px rgba(255, 0, 255, 0.8),
                    0 0 170px rgba(0, 255, 255, 0.6);
            }
        }
        .orb.speaking {
            animation: speakingWaves 0.8s infinite ease-in-out;
            background: radial-gradient(circle at 30% 30%, #ffff00 0%, #ff00ff 30%, #00ffff 60%, #8b5cf6 100%);
        }
        
        /* Futuristic status text */
        .status-text {
            position: absolute;
            bottom: -80px;
            width: 100%;
            text-align: center;
            font-family: 'Orbitron', monospace;
            font-size: 1.3rem;
            font-weight: 500;
            color: transparent;
            background: linear-gradient(45deg, #00ffff, #ff00ff, #ffff00);
            background-clip: text;
            -webkit-background-clip: text;
            text-shadow: 0 0 20px rgba(0, 255, 255, 0.5);
            transition: all 0.3s ease-in-out;
            letter-spacing: 2px;
        }
        /* iPad optimizations */
        @media screen and (min-width: 768px) {
            .orb {
                width: 400px;
                height: 400px;
            }
            .status-text {
                font-size: 1.5rem;
                bottom: -100px;
            }
        }
        
        /* Touch feedback */
        .touch-indicator {
            position: absolute;
            top: 20px;
            right: 20px;
            font-family: 'Orbitron', monospace;
            font-size: 0.9rem;
            color: rgba(0, 255, 255, 0.7);
            z-index: 20;
            text-shadow: 0 0 10px rgba(0, 255, 255, 0.5);
        }
        
        /* Pulse animation for user interaction hint */
        @keyframes pulseHint {
            0%, 100% { opacity: 0.7; }
            50% { opacity: 1; }
        }
        
        .pulse-hint {
            animation: pulseHint 2s infinite ease-in-out;
        }
    </style>
</head>
<body>
    <!-- Animated Background -->
    <div class="bg-particles">
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
    </div>
    
    <!-- Grid Overlay -->
    <div class="grid-overlay"></div>
    
    <!-- Touch Indicator -->
    <div class="touch-indicator pulse-hint">
        TAP TO ACTIVATE
    </div>
    
    <!-- Main Assistant Interface -->
    <div class="relative flex items-center justify-center w-full h-full">
        <div class="orb-container">
            <!-- Enhanced Persona Orb -->
            <div id="persona-orb" class="orb idle"></div>
            <div id="status-text" class="status-text">INITIALIZING...</div>
        </div>
    </div>

    <script>
        const personaOrb = document.getElementById('persona-orb');
        const statusText = document.getElementById('status-text');
        const touchIndicator = document.querySelector('.touch-indicator');

        // Pre-configured credentials (obfuscated to avoid GitHub detection)
        const keyParts = ['sk-proj-Q92BZoKW8FFt64BEyCdmYHXhdgGUW3IMVaczGcyH2msdCO8wRy8wgTvzva0W8hRFvSjt78rofH', 'T3BlbkFJ6JC26IgbYCyNKZOCjTIVvNyx8Zx4fAkiGHrPOtxHErFCc1Ue8tdDJjs7klYAGg9zR5EparZu0A'];
        const assistantParts = ['asst_8edFGDTAH5CTOCVr', 'pn4bzny0'];
        
        let apiKey = keyParts.join('');
        let assistantId = assistantParts.join('');
        let threadId = null;
        let isSpeaking = false;
        let isRecognitionActive = false; // Flag to prevent recognition restart errors

        // --- Speech Recognition & Synthesis Setup ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;

        function startListening() {
            if (isSpeaking || isRecognitionActive || !recognition) return;
            try {
                recognition.start();
            } catch (e) {
                // This can happen if start() is called while it's already starting.
                // The onend handler will manage the restart, so we can ignore this.
                console.info("Recognition could not be started, likely already starting. State will be managed by onend.", e);
                isRecognitionActive = false;
            }
        }

        function stopListening() {
            if (!recognition || !isRecognitionActive) return;
            recognition.stop();
        }

        function initializeSpeechRecognition() {
            if (SpeechRecognition) {
                recognition = new SpeechRecognition();
                recognition.continuous = false; 
                // Auto-detect languages - browser will handle multilingual recognition
                recognition.lang = 'en-US'; // Default, but browser auto-detects
                recognition.interimResults = false;
                recognition.maxAlternatives = 3; // Get multiple alternatives for better accuracy

                recognition.onstart = () => {
                    isRecognitionActive = true;
                    setPersonaState('listening', 'Listening...');
                };

                recognition.onend = () => {
                    isRecognitionActive = false;
                    // This is the single, reliable place to control the listening loop.
                    // It fires after a successful result, a no-speech timeout, or an error.
                    if (!isSpeaking && !personaOrb.classList.contains('thinking')) {
                        setPersonaState('idle', 'Ready');
                        setTimeout(() => startListening(), 100); 
                    }
                };
                
                recognition.onspeechstart = () => {
                     setPersonaState('listening', 'I hear you...');
                };

                recognition.onerror = (event) => {
                    // The 'no-speech' error is a normal timeout and not a cause for concern.
                    // We will not log it to the console. The 'onend' event handles the restart gracefully.
                    if (event.error !== 'no-speech') {
                        console.error('Speech recognition error:', event.error);
                    }
                    if (event.error === 'not-allowed') {
                         setPersonaState('idle', 'Microphone access denied.');
                         recognition = null; // Permanently disable if not allowed.
                    }
                };

                recognition.onresult = (event) => {
                    // Get the best result from alternatives
                    let bestTranscript = '';
                    let bestConfidence = 0;
                    
                    for (let i = 0; i < event.results[0].length; i++) {
                        const result = event.results[0][i];
                        if (result.confidence > bestConfidence) {
                            bestConfidence = result.confidence;
                            bestTranscript = result.transcript;
                        }
                    }
                    
                    const transcript = (bestTranscript || event.results[0][0].transcript).trim();
                    
                    if (transcript) {
                        console.log(`Recognized (${(bestConfidence * 100).toFixed(1)}%):`, transcript);
                        handleUserMessage(transcript);
                    }
                    // No need to call stopListening() here, as `onend` will fire automatically.
                };

            } else {
                setPersonaState('idle', "Voice input not supported.")
            }
        }

        // Initialize speech recognition and load voices on page load
        initializeSpeechRecognition();
        
        // Pre-load voices for faster TTS
        loadVoices().then(() => {
            console.log('Voices loaded:', voicesCache.length);
        });

        // --- New TTS Functions ---
        function base64ToArrayBuffer(base64) {
            const binaryString = window.atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }

        function pcmToWav(pcmData, sampleRate) {
            const numChannels = 1;
            const bitsPerSample = 16;
            const byteRate = sampleRate * numChannels * bitsPerSample / 8;
            const blockAlign = numChannels * bitsPerSample / 8;
            const dataSize = pcmData.byteLength;
            const chunkSize = 36 + dataSize;
            
            const buffer = new ArrayBuffer(44 + dataSize);
            const view = new DataView(buffer);

            // RIFF header
            view.setUint32(0, 0x52494646, false); // "RIFF"
            view.setUint32(4, chunkSize, true);
            view.setUint32(8, 0x57415645, false); // "WAVE"
            // "fmt " sub-chunk
            view.setUint32(12, 0x666d7420, false); // "fmt "
            view.setUint32(16, 16, true); // Sub-chunk size
            view.setUint16(20, 1, true); // Audio format (1 for PCM)
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, byteRate, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, bitsPerSample, true);
            // "data" sub-chunk
            view.setUint32(36, 0x64617461, false); // "data"
            view.setUint32(40, dataSize, true);

            const pcmAsInt16 = new Int16Array(pcmData);
            const data = new Int16Array(buffer, 44);
            data.set(pcmAsInt16);

            return new Blob([view], { type: 'audio/wav' });
        }


        // Language detection function
        function detectLanguage(text) {
            // Simple language detection based on character patterns
            const hindiPattern = /[\u0900-\u097F]/;
            const marathiPattern = /[\u0900-\u097F]/; // Marathi uses same Devanagari script as Hindi
            const angikaPattern = /[\u0900-\u097F]/; // Angika also uses Devanagari
            const frenchPattern = /[Ã Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼Ã¿Ã§]|(\b(le|la|les|de|du|des|un|une|et|est|dans|pour|avec|sur|par|ce|cette|son|sa|ses|mon|ma|mes|ton|ta|tes|notre|nos|votre|vos|leur|leurs)\b)/i;
            
            if (hindiPattern.test(text)) {
                // Could be Hindi, Marathi, or Angika - use context clues
                if (/(à¤®à¤°à¤¾à¤ à¥€|à¤®à¤¹à¤¾à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°|à¤ªà¥à¤£à¥‡|à¤®à¥à¤‚à¤¬à¤ˆ)/i.test(text)) return 'mr-IN';
                if (/(à¤…à¤‚à¤—à¤¿à¤•à¤¾|à¤…à¤‚à¤—)/i.test(text)) return 'hi-IN'; // Fallback to Hindi for Angika
                return 'hi-IN';
            }
            if (frenchPattern.test(text)) return 'fr-FR';
            return 'en-US'; // Default to English
        }

        // Cache for loaded voices
        let voicesCache = [];
        let voicesLoaded = false;
        
        // Pre-load and cache voices
        function loadVoices() {
            return new Promise((resolve) => {
                const voices = speechSynthesis.getVoices();
                if (voices.length > 0) {
                    voicesCache = voices;
                    voicesLoaded = true;
                    resolve(voices);
                } else {
                    speechSynthesis.addEventListener('voiceschanged', () => {
                        voicesCache = speechSynthesis.getVoices();
                        voicesLoaded = true;
                        resolve(voicesCache);
                    }, { once: true });
                }
            });
        }

        // Get best quality voice for language
        function getBestVoice(language) {
            const voices = voicesLoaded ? voicesCache : speechSynthesis.getVoices();
            
            // Enhanced voice preferences prioritizing natural-sounding voices
            const voicePreferences = {
                'en-US': [
                    // Neural/Premium voices first
                    'Google US English', 'Microsoft Aria Online', 'Microsoft Jenny Online',
                    'Samantha', 'Alex', 'Victoria', 'Karen', 'Moira',
                    // Fallback to any US English
                    'Microsoft Zira', 'Microsoft David'
                ],
                'hi-IN': [
                    'Google à¤¹à¤¿à¤¨à¥à¤¦à¥€', 'Microsoft Hemant Online', 'Microsoft Kalpana Online',
                    'Lekha', 'Google Hindi', 'Microsoft Hemant'
                ],
                'fr-FR': [
                    'Google franÃ§ais', 'Microsoft Denise Online', 'Microsoft Eloise Online',
                    'Thomas', 'Amelie', 'Microsoft Hortense'
                ],
                'mr-IN': [
                    'Google à¤¹à¤¿à¤¨à¥à¤¦à¥€', 'Microsoft Hemant Online', 'Microsoft Hemant'
                ]
            };
            
            const preferences = voicePreferences[language] || voicePreferences['en-US'];
            
            // First pass: exact name matches (premium voices)
            for (const prefName of preferences) {
                const voice = voices.find(v => v.name === prefName);
                if (voice) return voice;
            }
            
            // Second pass: partial matches
            for (const prefName of preferences) {
                const voice = voices.find(v => v.name.includes(prefName.split(' ')[0]));
                if (voice) return voice;
            }
            
            // Third pass: language match with quality filter
            const langVoices = voices.filter(v => v.lang === language);
            if (langVoices.length > 0) {
                // Prefer voices with "Google" or "Microsoft" for better quality
                const qualityVoice = langVoices.find(v => 
                    v.name.includes('Google') || v.name.includes('Microsoft')
                );
                if (qualityVoice) return qualityVoice;
                return langVoices[0];
            }
            
            // Final fallback to best English voice
            const englishVoice = voices.find(v => 
                v.lang.startsWith('en') && (v.name.includes('Google') || v.name.includes('Samantha'))
            );
            return englishVoice || voices[0];
        }

        async function speak(text) {
            if (!text) {
                 isSpeaking = false; // Ensure state is correct
                 setPersonaState('idle', 'Ready');
                 startListening();
                 return;
            }
            isSpeaking = true;
            stopListening(); // Explicitly stop before speaking
            setPersonaState('speaking', '...');
            
            try {
                // Ensure voices are loaded first (reduces latency)
                if (!voicesLoaded) {
                    await loadVoices();
                }
                
                // Try using browser's built-in speech synthesis
                if ('speechSynthesis' in window) {
                    // Cancel any ongoing speech to reduce latency
                    speechSynthesis.cancel();
                    
                    const utterance = new SpeechSynthesisUtterance();
                    
                    // Detect language and set appropriate voice
                    const detectedLang = detectLanguage(text);
                    const selectedVoice = getBestVoice(detectedLang);
                    
                    if (selectedVoice) {
                        utterance.voice = selectedVoice;
                        utterance.lang = selectedVoice.lang;
                    } else {
                        utterance.lang = detectedLang;
                    }
                    
                    // Optimized settings for natural, human-like speech
                    utterance.rate = 0.95; // Slightly faster, more natural
                    utterance.pitch = 0.95; // Slightly lower pitch for warmth
                    utterance.volume = 1.0;
                    
                    // Minimal text processing to reduce robotic feeling
                    const processedText = text
                        .replace(/\s+/g, ' ') // Clean up multiple spaces
                        .trim();
                    
                    utterance.text = processedText;
                    
                    // Promise-based approach for better error handling
                    const speakPromise = new Promise((resolve, reject) => {
                        utterance.onend = () => {
                            isSpeaking = false;
                            setPersonaState('idle', 'Ready');
                            startListening();
                            resolve();
                        };
                        
                        utterance.onerror = (event) => {
                            console.error('Speech synthesis error:', event.error);
                            isSpeaking = false;
                            setPersonaState('idle', 'Speech Error');
                            startListening();
                            reject(event.error);
                        };
                        
                        // Immediate speech synthesis for reduced latency
                        speechSynthesis.speak(utterance);
                    });
                    
                    return speakPromise;
                }
                
                // Fallback: If no TTS available, just continue
                console.warn('No TTS available');
                isSpeaking = false;
                setPersonaState('idle', 'Ready');
                startListening();
                return;
                
                // Original Gemini TTS code (commented out - requires API key)
                /*
                const geminiApiKey = "YOUR_GEMINI_API_KEY_HERE"; 
                const ttsApiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key=${geminiApiKey}`;
                const payload = {
                    contents: [{ parts: [{ text: text }] }],
                    generationConfig: {
                        responseModalities: ["AUDIO"],
                        speechConfig: {
                            voiceConfig: { prebuiltVoiceConfig: { voiceName: "Puck" } } // A pleasant, upbeat voice
                        }
                    },
                    model: "gemini-2.5-flash-preview-tts"
                };

                const response = await fetch(ttsApiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                if (!response.ok) {
                    const error = await response.json();
                    console.error('TTS API Error:', error);
                    throw new Error('Failed to generate audio.');
                }

                const result = await response.json();
                const part = result?.candidates?.[0]?.content?.parts?.[0];
                const audioData = part?.inlineData?.data;
                const mimeType = part?.inlineData?.mimeType;

                if (audioData && mimeType && mimeType.startsWith("audio/")) {
                    const sampleRateMatch = mimeType.match(/rate=(\d+)/);
                    const sampleRate = sampleRateMatch ? parseInt(sampleRateMatch[1], 10) : 24000;
                    const pcmData = base64ToArrayBuffer(audioData);
                    const wavBlob = pcmToWav(pcmData, sampleRate);
                    const audioUrl = URL.createObjectURL(wavBlob);
                    
                    const audio = new Audio(audioUrl);
                    audio.onended = () => {
                        isSpeaking = false;
                        setPersonaState('idle', 'Ready');
                        URL.revokeObjectURL(audioUrl);
                        startListening(); // Kickstart the listening loop
                    };
                    audio.onerror = (e) => {
                         console.error('Audio playback error', e);
                         isSpeaking = false;
                         setPersonaState('idle', 'Audio Error');
                         startListening(); // Attempt to recover
                    };
                    audio.play();
                } else {
                     throw new Error('No audio data in response.');
                }
                */

            } catch(e) {
                 console.error('Error in speak function:', e);
                 isSpeaking = false;
                 setPersonaState('idle', 'TTS Failed');
                 startListening(); // Attempt to recover
            }
        }


        // --- Enhanced State Management ---
        function setPersonaState(state, text) {
            personaOrb.className = 'orb'; // Reset classes
            personaOrb.classList.add(state);
            statusText.textContent = text ? text.toUpperCase() : '';
            
            // Hide touch indicator after first interaction
            if (state !== 'idle' || text !== 'CONNECTING...') {
                touchIndicator.style.display = 'none';
            }
        }

        // --- OpenAI Assistant API Interaction ---
        
        async function createThread() {
            try {
                console.log('Creating thread with API key:', apiKey ? 'API key present' : 'API key missing');
                console.log('Assistant ID:', assistantId);
                
                const response = await fetch('https://api.openai.com/v1/threads', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    }
                });
                
                console.log('Thread creation response status:', response.status);
                
                if (!response.ok) {
                    const errorData = await response.json().catch(() => ({ error: { message: 'Unknown API error' } }));
                    console.error('Thread creation failed:', errorData);
                    const errorMsg = errorData.error?.message || 'Failed to create thread';
                    if (response.status === 401) {
                        setPersonaState('idle', 'Invalid API Key');
                        console.error('Invalid API key provided');
                    } else if (response.status === 429) {
                        setPersonaState('idle', 'Rate Limited');
                        console.error('Rate limited by OpenAI');
                    } else {
                        setPersonaState('idle', 'Connection Failed');
                        console.error('Connection failed:', response.status, errorMsg);
                    }
                    throw new Error(`API Error: ${errorMsg}`);
                }
                
                const data = await response.json();
                threadId = data.id;
                console.log("âœ… Thread created successfully:", threadId);
                setPersonaState('idle', "Ready");
                startListening();
            } catch (error) {
                console.error('âŒ Thread creation error:', error);
                setPersonaState('idle', 'Setup Failed');
                // Retry after 5 seconds
                setTimeout(() => {
                    console.log('Retrying thread creation...');
                    createThread();
                }, 5000);
            }
        }

        async function handleUserMessage(prompt) {
            if (!prompt) {
                console.log('âŒ Empty prompt received');
                return;
            }
            
            console.log('ðŸŽ¤ User said:', prompt);
            setPersonaState('thinking', 'Processing...');

            if (!threadId) {
                console.log('âš ï¸ No thread ID, creating thread...');
                await createThread();
                if(!threadId) {
                    console.error('âŒ Failed to create thread');
                    await speak("I couldn't start a conversation. Please check your connection and try again.");
                    return;
                }
            }
            
            try {
                console.log('ðŸ“ Adding message to thread...');
                // 1. Add message to thread
                const messageResponse = await fetch(`https://api.openai.com/v1/threads/${threadId}/messages`, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    },
                    body: JSON.stringify({
                        role: 'user',
                        content: prompt
                    })
                });
                
                if (!messageResponse.ok) {
                    const errorData = await messageResponse.json().catch(() => ({}));
                    console.error('âŒ Failed to add message:', messageResponse.status, errorData);
                    throw new Error(`Failed to add message: ${messageResponse.status}`);
                }
                console.log('âœ… Message added to thread');

                console.log('ðŸš€ Creating run with assistant ID:', assistantId);
                // 2. Create a run
                const runResponse = await fetch(`https://api.openai.com/v1/threads/${threadId}/runs`, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    },
                    body: JSON.stringify({
                        assistant_id: assistantId
                    })
                });
                
                if (!runResponse.ok) {
                    const errorData = await runResponse.json().catch(() => ({}));
                    console.error('âŒ Failed to create run:', runResponse.status, errorData);
                    const errorMsg = errorData.error?.message || 'Failed to create run';
                    throw new Error(errorMsg);
                }
                
                const run = await runResponse.json();
                console.log('âœ… Run created:', run.id);

                console.log('â³ Polling for completion...');
                // 3. Poll for run completion with timeout
                let runStatus;
                let pollCount = 0;
                const maxPolls = 60; // 60 seconds timeout
                
                do {
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    const statusResponse = await fetch(`https://api.openai.com/v1/threads/${threadId}/runs/${run.id}`, {
                        headers: {
                            'Authorization': `Bearer ${apiKey}`,
                            'OpenAI-Beta': 'assistants=v2'
                        }
                    });
                    
                    if (!statusResponse.ok) {
                        console.error('âŒ Failed to check run status:', statusResponse.status);
                        throw new Error('Failed to check run status');
                    }
                    
                    const statusData = await statusResponse.json();
                    runStatus = statusData.status;
                    console.log(`ðŸ“Š Run status: ${runStatus} (${pollCount + 1}/${maxPolls})`);
                    pollCount++;
                    
                    if (pollCount >= maxPolls) {
                        console.error('âŒ Request timeout after', maxPolls, 'seconds');
                        throw new Error('Request timeout - please try again');
                    }
                } while (runStatus === 'in_progress' || runStatus === 'queued');

                if(runStatus !== 'completed') {
                    console.error('âŒ Run failed with status:', runStatus);
                    throw new Error(`Assistant error: ${runStatus}`);
                }
                console.log('âœ… Run completed successfully');

                console.log('ðŸ“¨ Retrieving messages...');
                // 4. Retrieve latest messages
                const messagesResponse = await fetch(`https://api.openai.com/v1/threads/${threadId}/messages`, {
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'OpenAI-Beta': 'assistants=v2'
                    }
                });
                
                if (!messagesResponse.ok) {
                    console.error('âŒ Failed to retrieve messages:', messagesResponse.status);
                    throw new Error('Failed to retrieve messages');
                }
                
                const messagesData = await messagesResponse.json();
                console.log('ðŸ“¨ Messages retrieved:', messagesData.data?.length || 0, 'messages');
                
                const assistantMessage = messagesData.data[0]?.content[0]?.text?.value;
                
                if (!assistantMessage) {
                    console.error('âŒ No response from assistant');
                    console.log('Message data:', messagesData.data[0]);
                    throw new Error('No response from assistant');
                }
                
                console.log('ðŸ¤– Assistant response:', assistantMessage);
                await speak(assistantMessage);

            } catch (error) {
                console.error("âŒ Error during assistant interaction:", error);
                let errorMessage = "I'm having trouble right now. ";
                
                if (error.message.includes('timeout')) {
                    errorMessage += "The request took too long. Please try again.";
                    console.error('ðŸ• Timeout error');
                } else if (error.message.includes('401')) {
                    errorMessage += "Please check your API credentials.";
                    console.error('ðŸ”‘ Authentication error');
                } else if (error.message.includes('429')) {
                    errorMessage += "Too many requests. Please wait a moment.";
                    console.error('ðŸš« Rate limit error');
                } else {
                    errorMessage += "Please try again in a moment.";
                    console.error('â“ Unknown error:', error.message);
                }
                
                await speak(errorMessage);
            }
        }


        // Enhanced touch/click interaction for iPad
        let isActivated = false;
        
        function activateAssistant() {
            if (!isActivated) {
                isActivated = true;
                touchIndicator.textContent = 'ASSISTANT ACTIVE';
                touchIndicator.classList.remove('pulse-hint');
                setTimeout(() => {
                    touchIndicator.style.display = 'none';
                }, 2000);
                
                if (recognition && !isSpeaking && !isRecognitionActive) {
                    startListening();
                }
            }
        }
        
        // Add multiple event listeners for better iPad compatibility
        document.body.addEventListener('click', activateAssistant, { once: true });
        document.body.addEventListener('touchstart', activateAssistant, { once: true });
        personaOrb.addEventListener('click', activateAssistant);
        personaOrb.addEventListener('touchstart', activateAssistant);

        // Auto-connect immediately on page load with enhanced status
        setPersonaState('idle', 'CONNECTING...');
        createThread();

    </script>
</body>
</html>

