<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Persona</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Exo+2:wght@300;400;500;600;700&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Exo 2', sans-serif;
            overflow: hidden;
            background: radial-gradient(ellipse at center, #1a1a2e 0%, #16213e 30%, #0f3460 60%, #0c0c0c 100%);
            position: relative;
            width: 100vw;
            height: 100vh;
        }
        
        /* Animated background particles */
        .bg-particles {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            overflow: hidden;
            z-index: 1;
        }
        
        .particle {
            position: absolute;
            background: radial-gradient(circle, rgba(0, 255, 255, 0.6), rgba(255, 0, 255, 0.3), transparent 70%);
            border-radius: 50%;
            animation: float 8s infinite ease-in-out;
            filter: blur(1px);
        }
        
        .particle:nth-child(1) { width: 4px; height: 4px; top: 20%; left: 10%; animation-delay: 0s; }
        .particle:nth-child(2) { width: 6px; height: 6px; top: 80%; left: 20%; animation-delay: 1s; }
        .particle:nth-child(3) { width: 3px; height: 3px; top: 40%; left: 80%; animation-delay: 2s; }
        .particle:nth-child(4) { width: 5px; height: 5px; top: 60%; left: 90%; animation-delay: 3s; }
        .particle:nth-child(5) { width: 4px; height: 4px; top: 10%; left: 60%; animation-delay: 4s; }
        .particle:nth-child(6) { width: 7px; height: 7px; top: 90%; left: 70%; animation-delay: 5s; }
        
        @keyframes float {
            0%, 100% { 
                transform: translateY(0px) translateX(0px) rotate(0deg) scale(1); 
                opacity: 0.8; 
            }
            25% { 
                transform: translateY(-25px) translateX(10px) rotate(90deg) scale(1.2); 
                opacity: 0.6; 
            }
            50% { 
                transform: translateY(-15px) translateX(-5px) rotate(180deg) scale(0.8); 
                opacity: 0.4; 
            }
            75% { 
                transform: translateY(-35px) translateX(15px) rotate(270deg) scale(1.1); 
                opacity: 0.7; 
            }
        }
        
        /* Animated grid overlay with curves */
        .grid-overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-image: 
                radial-gradient(circle at center, rgba(0, 255, 255, 0.08) 1px, transparent 2px),
                linear-gradient(45deg, rgba(255, 0, 255, 0.03) 1px, transparent 1px),
                linear-gradient(-45deg, rgba(0, 255, 255, 0.03) 1px, transparent 1px);
            background-size: 60px 60px, 40px 40px, 40px 40px;
            z-index: 2;
            animation: gridPulse 6s ease-in-out infinite;
            filter: blur(0.5px);
        }
        
        @keyframes gridPulse {
            0%, 100% { 
                opacity: 0.4; 
                transform: scale(1);
            }
            50% { 
                opacity: 0.2; 
                transform: scale(1.02);
            }
        }

        /* --- Enhanced Futuristic Orb --- */
        .orb-container {
            position: relative;
            z-index: 10;
        }
        
        .orb {
            width: 320px;
            height: 320px;
            border-radius: 50%;
            position: relative;
            background: 
                radial-gradient(ellipse at 25% 25%, rgba(0, 255, 255, 0.4) 0%, transparent 50%),
                radial-gradient(ellipse at 75% 75%, rgba(255, 0, 255, 0.3) 0%, transparent 50%),
                radial-gradient(circle at center, #1a1a2e 0%, #16213e 40%, #0f3460 80%, #533a7b 100%);
            box-shadow: 
                0 0 40px rgba(0, 255, 255, 0.4),
                0 0 70px rgba(255, 0, 255, 0.3),
                0 0 100px rgba(139, 92, 246, 0.2),
                inset 0 0 60px rgba(255, 255, 255, 0.05),
                inset 0 0 20px rgba(0, 255, 255, 0.1);
            transition: all 1.2s cubic-bezier(0.25, 0.46, 0.45, 0.94);
            border: 1px solid rgba(0, 255, 255, 0.2);
            backdrop-filter: blur(10px);
        }
        
        .orb::before {
            content: '';
            position: absolute;
            top: -8px;
            left: -8px;
            right: -8px;
            bottom: -8px;
            border-radius: 50%;
            background: conic-gradient(
                from 0deg, 
                transparent 0%, 
                rgba(0, 255, 255, 0.2) 25%, 
                transparent 50%, 
                rgba(255, 0, 255, 0.2) 75%, 
                transparent 100%
            );
            animation: orbRotate 12s linear infinite;
            z-index: -1;
            filter: blur(2px);
        }
        
        .orb::after {
            content: '';
            position: absolute;
            top: 30px;
            left: 30px;
            width: 80px;
            height: 80px;
            border-radius: 50%;
            background: radial-gradient(
                ellipse at 30% 30%, 
                rgba(255, 255, 255, 0.3) 0%, 
                rgba(0, 255, 255, 0.1) 40%,
                transparent 70%
            );
            filter: blur(25px);
            animation: shimmer 4s ease-in-out infinite;
        }
        
        @keyframes shimmer {
            0%, 100% { opacity: 0.6; transform: scale(1); }
            50% { opacity: 0.9; transform: scale(1.1); }
        }
        
        @keyframes orbRotate {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }

        /* IDLE state - gentle breathing */
        @keyframes idleBreath {
            0%, 100% { 
                transform: scale(1) rotate(0deg);
                box-shadow: 
                    0 0 40px rgba(0, 255, 255, 0.4),
                    0 0 70px rgba(255, 0, 255, 0.3),
                    0 0 100px rgba(139, 92, 246, 0.2);
            }
            50% { 
                transform: scale(1.03) rotate(1deg);
                box-shadow: 
                    0 0 50px rgba(0, 255, 255, 0.5),
                    0 0 80px rgba(255, 0, 255, 0.4),
                    0 0 110px rgba(139, 92, 246, 0.3);
            }
        }
        .orb.idle {
            animation: idleBreath 8s infinite ease-in-out;
            background: 
                radial-gradient(ellipse at 25% 25%, rgba(0, 204, 255, 0.3) 0%, transparent 50%),
                radial-gradient(ellipse at 75% 75%, rgba(204, 0, 255, 0.2) 0%, transparent 50%),
                radial-gradient(circle at center, #1a1a2e 0%, #16213e 40%, #0f3460 80%, #533a7b 100%);
        }
        
        /* LISTENING state - active pulsing */
        @keyframes listeningPulse {
            0%, 100% { 
                transform: scale(1.08) rotate(-1deg);
                box-shadow: 
                    0 0 60px rgba(0, 255, 255, 0.7),
                    0 0 90px rgba(255, 0, 255, 0.5),
                    0 0 120px rgba(139, 92, 246, 0.4);
            }
            50% { 
                transform: scale(1.15) rotate(1deg);
                box-shadow: 
                    0 0 80px rgba(0, 255, 255, 0.9),
                    0 0 110px rgba(255, 0, 255, 0.7),
                    0 0 140px rgba(139, 92, 246, 0.5);
            }
        }
        .orb.listening {
            animation: listeningPulse 1.5s infinite ease-in-out;
            background: 
                radial-gradient(ellipse at 25% 25%, rgba(0, 255, 255, 0.6) 0%, transparent 50%),
                radial-gradient(ellipse at 75% 75%, rgba(255, 0, 255, 0.4) 0%, transparent 50%),
                radial-gradient(circle at center, #1a1a2e 0%, #16213e 30%, #0f3460 70%, #3b82f6 100%);
        }

        /* THINKING state - spinning energy rings */
        @keyframes thinkingRings {
            0% { 
                transform: rotate(0deg) scale(0.95); 
                opacity: 0.8;
            }
            50% { 
                transform: rotate(180deg) scale(1.02); 
                opacity: 0.6;
            }
            100% { 
                transform: rotate(360deg) scale(0.95); 
                opacity: 0.8;
            }
        }
        .orb.thinking::before {
            animation: thinkingRings 3s ease-in-out infinite;
            background: conic-gradient(from 0deg, 
                transparent 0%, 
                rgba(0, 255, 255, 0.4) 20%, 
                transparent 40%, 
                rgba(255, 0, 255, 0.4) 60%, 
                transparent 80%,
                rgba(139, 92, 246, 0.3) 100%);
        }
        .orb.thinking {
            background: 
                radial-gradient(ellipse at 35% 35%, rgba(255, 0, 255, 0.4) 0%, transparent 50%),
                radial-gradient(ellipse at 65% 65%, rgba(0, 255, 255, 0.3) 0%, transparent 50%),
                radial-gradient(circle at center, #1a1a2e 0%, #16213e 30%, #533a7b 70%, #8b5cf6 100%);
            box-shadow: 
                0 0 60px rgba(255, 0, 255, 0.6),
                0 0 90px rgba(0, 255, 255, 0.4),
                0 0 120px rgba(139, 92, 246, 0.3);
            animation: thinkingPulse 2s ease-in-out infinite;
        }
        
        @keyframes thinkingPulse {
            0%, 100% { transform: scale(1.02) rotate(-0.5deg); }
            50% { transform: scale(1.06) rotate(0.5deg); }
        }

        /* SPEAKING state - dynamic waves */
        @keyframes speakingWaves {
            0%, 100% { 
                transform: scale(1.04) rotate(-1deg);
                box-shadow: 
                    0 0 50px rgba(255, 255, 0, 0.6),
                    0 0 80px rgba(255, 0, 255, 0.4),
                    0 0 110px rgba(0, 255, 255, 0.3);
            }
            25% { 
                transform: scale(1.12) rotate(0.5deg);
                box-shadow: 
                    0 0 70px rgba(255, 255, 0, 0.8),
                    0 0 100px rgba(255, 0, 255, 0.6),
                    0 0 130px rgba(0, 255, 255, 0.4);
            }
            50% { 
                transform: scale(1.08) rotate(1deg);
                box-shadow: 
                    0 0 60px rgba(255, 255, 0, 0.7),
                    0 0 90px rgba(255, 0, 255, 0.5),
                    0 0 120px rgba(0, 255, 255, 0.35);
            }
            75% { 
                transform: scale(1.16) rotate(-0.5deg);
                box-shadow: 
                    0 0 80px rgba(255, 255, 0, 0.9),
                    0 0 110px rgba(255, 0, 255, 0.7),
                    0 0 140px rgba(0, 255, 255, 0.5);
            }
        }
        .orb.speaking {
            animation: speakingWaves 1.2s infinite ease-in-out;
            background: 
                radial-gradient(ellipse at 30% 30%, rgba(255, 255, 0, 0.5) 0%, transparent 50%),
                radial-gradient(ellipse at 70% 70%, rgba(255, 0, 255, 0.3) 0%, transparent 50%),
                radial-gradient(circle at center, #1a1a2e 0%, #16213e 20%, #ffaa00 60%, #8b5cf6 100%);
        }
        
        /* Futuristic status text with curves */
        .status-text {
            position: absolute;
            bottom: -80px;
            width: 100%;
            text-align: center;
            font-family: 'Orbitron', monospace;
            font-size: 1.2rem;
            font-weight: 400;
            color: transparent;
            background: linear-gradient(
                135deg, 
                rgba(0, 255, 255, 0.8) 0%, 
                rgba(255, 0, 255, 0.6) 50%, 
                rgba(255, 255, 0, 0.4) 100%
            );
            background-clip: text;
            -webkit-background-clip: text;
            text-shadow: 
                0 0 15px rgba(0, 255, 255, 0.3),
                0 2px 10px rgba(255, 0, 255, 0.2);
            transition: all 0.6s cubic-bezier(0.25, 0.46, 0.45, 0.94);
            letter-spacing: 1.5px;
            animation: textGlow 3s ease-in-out infinite;
        }
        
        @keyframes textGlow {
            0%, 100% { 
                opacity: 0.8;
                transform: translateY(0px);
            }
            50% { 
                opacity: 1;
                transform: translateY(-2px);
            }
        }
        /* iPad optimizations */
        @media screen and (min-width: 768px) {
            .orb {
                width: 400px;
                height: 400px;
            }
            .status-text {
                font-size: 1.5rem;
                bottom: -100px;
            }
        }
        
        /* Touch feedback */
        .touch-indicator {
            position: absolute;
            top: 20px;
            right: 20px;
            font-family: 'Orbitron', monospace;
            font-size: 0.9rem;
            color: rgba(0, 255, 255, 0.7);
            z-index: 20;
            text-shadow: 0 0 10px rgba(0, 255, 255, 0.5);
        }
        
        /* System mode indicator */
        .mode-indicator {
            position: absolute;
            bottom: 20px;
            left: 20px;
            font-family: 'Orbitron', monospace;
            font-size: 0.8rem;
            color: rgba(0, 255, 255, 0.6);
            z-index: 20;
            text-shadow: 0 0 8px rgba(0, 255, 255, 0.4);
            opacity: 0;
            transition: opacity 0.5s ease-in-out;
        }
        
        .mode-indicator.visible {
            opacity: 1;
        }
        
        /* Pulse animation for user interaction hint */
        @keyframes pulseHint {
            0%, 100% { opacity: 0.7; }
            50% { opacity: 1; }
        }
        
        .pulse-hint {
            animation: pulseHint 2s infinite ease-in-out;
        }
    </style>
</head>
<body>
    <!-- Animated Background -->
    <div class="bg-particles">
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
        <div class="particle"></div>
    </div>
    
    <!-- Grid Overlay -->
    <div class="grid-overlay"></div>
    
    <!-- Touch Indicator -->
    <div class="touch-indicator pulse-hint">
        TAP TO ACTIVATE
    </div>
    
    <!-- System Mode Indicator -->
    <div class="mode-indicator" id="mode-indicator">
        ENHANCED AUDIO MODE
    </div>
    
    <!-- Main Assistant Interface -->
    <div class="relative flex items-center justify-center w-full h-full">
        <div class="orb-container">
            <!-- Enhanced Persona Orb -->
            <div id="persona-orb" class="orb idle"></div>
            <div id="status-text" class="status-text">INITIALIZING...</div>
        </div>
    </div>

    <script>
        /*
         * AI Voice Assistant - OpenAI Realtime API Implementation
         * 
         * This implementation uses OpenAI's Realtime API for true voice-to-voice interaction:
         * - Real-time audio streaming via WebSocket
         * - Server-side proxy handles authentication
         * - Low-latency voice conversations
         * - Advanced voice activity detection
         * - Seamless audio processing
         * 
         * Features:
         * - OpenAI Realtime API for voice-to-voice conversations
         * - WebSocket connection through authentication proxy
         * - Real-time audio streaming (PCM16 format)
         * - Server-side voice activity detection
         * - Futuristic cyberpunk visual interface
         */

        const personaOrb = document.getElementById('persona-orb');
        const statusText = document.getElementById('status-text');
        const touchIndicator = document.querySelector('.touch-indicator');
        const modeIndicator = document.getElementById('mode-indicator');

        // Pre-configured credentials (obfuscated to avoid GitHub detection)
        const keyParts = ['sk-proj-Q92BZoKW8FFt64BEyCdmYHXhdgGUW3IMVaczGcyH2msdCO8wRy8wgTvzva0W8hRFvSjt78rofH', 'T3BlbkFJ6JC26IgbYCyNKZOCjTIVvNyx8Zx4fAkiGHrPOtxHErFCc1Ue8tdDJjs7klYAGg9zR5EparZu0A'];
        const assistantParts = ['asst_8edFGDTAH5CTOCVr', 'pn4bzny0'];
        
        let apiKey = keyParts.join('');
        let assistantId = assistantParts.join('');
        
        // OpenAI Realtime API Configuration
        const PROXY_URL = 'ws://localhost:3001/realtime'; // Local proxy server
        const REALTIME_MODEL = 'gpt-4o-realtime-preview-2024-10-01';
        
        // Realtime API variables
        let realtimeSocket = null;
        let audioStream = null;
        let audioContext = null;
        let microphoneSource = null;
        let isConnected = false;
        let isListening = false;
        let isSpeaking = false;

        // --- OpenAI Realtime API Implementation ---
        
        // Initialize audio context and microphone
        async function initializeAudio() {
            try {
                console.log('üé§ Initializing audio...');
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                audioStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        sampleRate: 24000,
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: true
                    } 
                });
                console.log('‚úÖ Audio initialized');
                return true;
            } catch (error) {
                console.error('‚ùå Audio initialization failed:', error);
                setPersonaState('idle', 'Microphone access denied');
                return false;
            }
        }

        // Connect to OpenAI Realtime API through proxy
        async function connectToRealtimeAPI() {
            try {
                console.log('üîÑ Connecting to OpenAI Realtime API...');
                setPersonaState('idle', 'CONNECTING...');
                
                // Connect to proxy server
                const wsUrl = `${PROXY_URL}?model=${REALTIME_MODEL}`;
                realtimeSocket = new WebSocket(wsUrl);
                
                realtimeSocket.onopen = () => {
                    console.log('‚úÖ Connected to Realtime API via proxy');
                    
                    // Configure the session
                    realtimeSocket.send(JSON.stringify({
                        type: 'session.update',
                        session: {
                            modalities: ['text', 'audio'],
                            instructions: 'You are a helpful voice assistant. Keep responses conversational and concise. You are interacting through voice, so respond naturally as if speaking to someone.',
                            voice: 'alloy',
                            input_audio_format: 'pcm16',
                            output_audio_format: 'pcm16',
                            input_audio_transcription: {
                                model: 'whisper-1'
                            },
                            turn_detection: {
                                type: 'server_vad',
                                threshold: 0.5,
                                prefix_padding_ms: 300,
                                silence_duration_ms: 200
                            },
                            temperature: 0.8,
                            max_response_output_tokens: 4096
                        }
                    }));
                    
                    setPersonaState('idle', 'Ready');
                    isConnected = true;
                    startRealtimeAudio();
                };
                
                realtimeSocket.onmessage = handleRealtimeMessage;
                
                realtimeSocket.onerror = (error) => {
                    console.error('‚ùå Realtime WebSocket error:', error);
                    setPersonaState('idle', 'Connection failed');
                };
                
                realtimeSocket.onclose = (event) => {
                    console.log('üîå Realtime WebSocket closed:', event.code, event.reason);
                    setPersonaState('idle', 'Disconnected');
                    isConnected = false;
                };
                
            } catch (error) {
                console.error('‚ùå Failed to connect to Realtime API:', error);
                setPersonaState('idle', 'Connection error');
            }
        }

        // Start real-time audio streaming to Realtime API
        function startRealtimeAudio() {
            if (!isConnected || !audioStream || !audioContext) return;
            
            console.log('üé§ Starting real-time audio streaming...');
            
            // Create audio processing pipeline
            const source = audioContext.createMediaStreamSource(audioStream);
            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            
            processor.onaudioprocess = (event) => {
                if (!isConnected || !realtimeSocket || realtimeSocket.readyState !== WebSocket.OPEN) {
                    return;
                }
                
                const inputData = event.inputBuffer.getChannelData(0);
                const pcm16Data = new Int16Array(inputData.length);
                
                // Convert float32 to PCM16
                for (let i = 0; i < inputData.length; i++) {
                    pcm16Data[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
                }
                
                // Send audio data to Realtime API
                realtimeSocket.send(JSON.stringify({
                    type: 'input_audio_buffer.append',
                    audio: arrayBufferToBase64(pcm16Data.buffer)
                }));
            };
            
            source.connect(processor);
            processor.connect(audioContext.destination);
            
            microphoneSource = { source, processor };
            isListening = true;
            
            console.log('‚úÖ Real-time audio streaming active');
        }

        // Handle Realtime API messages
        function handleRealtimeMessage(event) {
            const message = JSON.parse(event.data);
            console.log('üì® Realtime message:', message.type);

            switch (message.type) {
                case 'session.created':
                    console.log('‚úÖ Realtime session created');
                    break;
                
                case 'input_audio_buffer.speech_started':
                    console.log('üé§ Speech started');
                    setPersonaState('listening', 'Listening...');
                    break;
                
                case 'input_audio_buffer.speech_stopped':
                    console.log('üé§ Speech stopped');
                    setPersonaState('thinking', 'Processing...');
                    break;
                
                case 'response.audio.delta':
                    if (message.delta) {
                        playAudioDelta(message.delta);
                    }
                    break;
                
                case 'response.audio.done':
                    console.log('üîä Audio response complete');
                    setPersonaState('idle', 'Ready');
                    break;
                
                case 'response.done':
                    console.log('‚úÖ Response complete');
                    setPersonaState('idle', 'Ready');
                    break;
                
                case 'error':
                    console.error('‚ùå Realtime API error:', message.error);
                    setPersonaState('idle', 'Error occurred');
                    break;
            }
        }

        // Start voice recording with activity detection
        let mediaRecorder = null;
        let recordingChunks = [];
        let isRecording = false;
        let silenceTimer = null;
        
        function startVoiceRecording() {
            if (!audioStream) return;
            
            try {
                mediaRecorder = new MediaRecorder(audioStream, {
                    mimeType: 'audio/webm;codecs=opus'
                });
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0) {
                        recordingChunks.push(event.data);
                    }
                };
                
                mediaRecorder.onstop = async () => {
                    if (recordingChunks.length > 0) {
                        const audioBlob = new Blob(recordingChunks, { type: 'audio/webm' });
                        recordingChunks = [];
                        await processAudioInput(audioBlob);
                    }
                };
                
                // Start continuous recording with voice activity detection
                setupVoiceActivityDetection();
                
            } catch (error) {
                console.error('‚ùå Failed to setup voice recording:', error);
            }
        }
        
        // Modern voice activity detection using Web Audio API
        function setupVoiceActivityDetection() {
            try {
                // Use existing audioContext if available, or create new one
                const vadAudioContext = audioContext || new (window.AudioContext || window.webkitAudioContext)();
                const source = vadAudioContext.createMediaStreamSource(audioStream);
                const analyser = vadAudioContext.createAnalyser();
                
                // Configure analyser for voice detection
                analyser.fftSize = 512;
                analyser.smoothingTimeConstant = 0.3;
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                
                source.connect(analyser);
                
                let isSpeechDetected = false;
                const speechThreshold = 25; // Lower threshold for better sensitivity
                const silenceDelay = 1000; // ms - shorter for better responsiveness
                
                function detectVoiceActivity() {
                    if (!isConnected) return;
                    
                    analyser.getByteFrequencyData(dataArray);
                    
                    // Focus on speech frequency range (85Hz - 255Hz is roughly index 4-12 for 512 FFT)
                    const speechRange = dataArray.slice(4, 32);
                    const average = speechRange.reduce((a, b) => a + b) / speechRange.length;
                    
                    // Debug audio levels every few frames
                    if (Math.random() < 0.01) { // Log ~1% of the time to avoid spam
                        console.log('üîä Audio level:', Math.round(average), 'threshold:', speechThreshold);
                    }
                    
                    if (average > speechThreshold && !isRecording && !isSpeaking) {
                        // Speech started
                        console.log('üé§ Speech detected, starting recording...', `(level: ${Math.round(average)})`);
                        isSpeechDetected = true;
                        isRecording = true;
                        setPersonaState('listening', 'Listening...');
                        
                        recordingChunks = [];
                        mediaRecorder.start();
                        
                        clearTimeout(silenceTimer);
                        
                    } else if (average <= speechThreshold && isRecording) {
                        // Potential silence
                        clearTimeout(silenceTimer);
                        silenceTimer = setTimeout(() => {
                            if (isRecording) {
                                console.log('üîá Silence detected, stopping recording...', `(level: ${Math.round(average)})`);
                                isRecording = false;
                                isSpeechDetected = false;
                                setPersonaState('thinking', 'Processing...');
                                mediaRecorder.stop();
                            }
                        }, silenceDelay);
                    }
                    
                    if (isConnected && !isSpeaking) {
                        requestAnimationFrame(detectVoiceActivity);
                    }
                }
                
                detectVoiceActivity();
                
            } catch (error) {
                console.error('‚ùå Failed to setup voice activity detection:', error);
            }
        }

        // Process audio input using enhanced Audio API (Whisper + Assistant + TTS)
        async function processAudioInput(audioBlob) {
            try {
                console.log('üéµ Processing audio input via enhanced API...');
                
                // Step 1: Convert audio to text using Whisper
                const transcription = await transcribeAudio(audioBlob);
                if (!transcription) {
                    console.error('‚ùå No transcription received');
                    setPersonaState('idle', 'Ready');
                    return;
                }
                
                console.log('üìù Transcription:', transcription);
                
                // Step 2: Send to Assistant API
                const response = await sendToAssistant(transcription);
                if (!response) {
                    console.error('‚ùå No response from assistant');
                    setPersonaState('idle', 'Ready');
                    return;
                }
                
                console.log('üí≠ Assistant response:', response);
                
                // Step 3: Convert response to speech using OpenAI TTS
                await speakResponse(response);
                
            } catch (error) {
                console.error('‚ùå Error processing audio input:', error);
                setPersonaState('idle', 'Error occurred');
            }
        }

        // Transcribe audio using OpenAI Whisper
        async function transcribeAudio(audioBlob) {
            try {
                console.log('üé§ Transcribing audio...');
                
                const formData = new FormData();
                formData.append('file', audioBlob, 'audio.webm');
                formData.append('model', 'whisper-1');
                formData.append('language', 'en'); // Support for multilingual can be added later
                
                const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`
                    },
                    body: formData
                });
                
                if (!response.ok) {
                    throw new Error(`Whisper API error: ${response.status}`);
                }
                
                const data = await response.json();
                return data.text;
                
            } catch (error) {
                console.error('‚ùå Transcription failed:', error);
                return null;
            }
        }

        // Send message to Assistant API
        async function sendToAssistant(message) {
            try {
                console.log('ü§ñ Sending to assistant...');
                
                // Add message to thread
                const messageResponse = await fetch(`https://api.openai.com/v1/threads/${window.threadId}/messages`, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    },
                    body: JSON.stringify({
                        role: 'user',
                        content: message
                    })
                });
                
                if (!messageResponse.ok) {
                    throw new Error(`Failed to add message: ${messageResponse.status}`);
                }
                
                // Run the assistant
                const runResponse = await fetch(`https://api.openai.com/v1/threads/${window.threadId}/runs`, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json',
                        'OpenAI-Beta': 'assistants=v2'
                    },
                    body: JSON.stringify({
                        assistant_id: assistantId
                    })
                });
                
                if (!runResponse.ok) {
                    throw new Error(`Failed to start run: ${runResponse.status}`);
                }
                
                const runData = await runResponse.json();
                
                // Poll for completion
                const result = await pollRunCompletion(runData.id);
                return result;
                
            } catch (error) {
                console.error('‚ùå Assistant API failed:', error);
                return null;
            }
        }

        // Poll for run completion
        async function pollRunCompletion(runId) {
            const maxAttempts = 30;
            let attempts = 0;
            
            while (attempts < maxAttempts) {
                try {
                    const response = await fetch(`https://api.openai.com/v1/threads/${window.threadId}/runs/${runId}`, {
                        headers: {
                            'Authorization': `Bearer ${apiKey}`,
                            'Content-Type': 'application/json',
                            'OpenAI-Beta': 'assistants=v2'
                        }
                    });
                    
                    if (!response.ok) {
                        throw new Error(`Failed to check run status: ${response.status}`);
                    }
                    
                    const runData = await response.json();
                    
                    if (runData.status === 'completed') {
                        // Get the latest message
                        const messagesResponse = await fetch(`https://api.openai.com/v1/threads/${window.threadId}/messages`, {
                            headers: {
                                'Authorization': `Bearer ${apiKey}`,
                                'Content-Type': 'application/json',
                                'OpenAI-Beta': 'assistants=v2'
                            }
                        });
                        
                        if (messagesResponse.ok) {
                            const messagesData = await messagesResponse.json();
                            const latestMessage = messagesData.data[0];
                            if (latestMessage.role === 'assistant') {
                                return latestMessage.content[0].text.value;
                            }
                        }
                        return null;
                        
                    } else if (runData.status === 'failed' || runData.status === 'cancelled') {
                        console.error('‚ùå Run failed or cancelled:', runData.status);
                        return null;
                    }
                    
                    // Wait before next poll
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    attempts++;
                    
                } catch (error) {
                    console.error('‚ùå Error polling run:', error);
                    return null;
                }
            }
            
            console.error('‚ùå Run polling timed out');
            return null;
        }

        // Convert text to speech using OpenAI TTS
        async function speakResponse(text) {
            try {
                console.log('üîä Converting to speech...');
                setPersonaState('speaking', 'Speaking...');
                
                const response = await fetch('https://api.openai.com/v1/audio/speech', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${apiKey}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        model: 'tts-1',
                        input: text,
                        voice: 'nova', // High-quality voice
                        response_format: 'mp3'
                    })
                });
                
                if (!response.ok) {
                    throw new Error(`TTS API error: ${response.status}`);
                }
                
                const audioBuffer = await response.arrayBuffer();
                const audioBlob = new Blob([audioBuffer], { type: 'audio/mp3' });
                const audioUrl = URL.createObjectURL(audioBlob);
                
                const audio = new Audio(audioUrl);
                audio.onplay = () => {
                    console.log('üîä Starting audio playback');
                    isSpeaking = true;
                };
                
                audio.onended = () => {
                    console.log('‚úÖ Audio playback complete');
                    isSpeaking = false;
                    setPersonaState('idle', 'Ready');
                    URL.revokeObjectURL(audioUrl);
                };
                
                audio.onerror = (error) => {
                    console.error('‚ùå Audio playback error:', error);
                    isSpeaking = false;
                    setPersonaState('idle', 'Ready');
                    URL.revokeObjectURL(audioUrl);
                };
                
                await audio.play();
                
            } catch (error) {
                console.error('‚ùå TTS failed:', error);
                setPersonaState('idle', 'TTS Error');
            }
        }

        // Legacy Realtime API message handler (for future server-side proxy implementation)
        function handleRealtimeMessage(event) {
            console.log('üì® Realtime message handler called (legacy)');
            // This would be used with a server-side proxy for Realtime API
        }

        // Legacy function - Realtime API listening no longer used in browser
        // Keeping for potential future server-side proxy implementation
        function startRealtimeListening() {
            console.log('‚ö†Ô∏è startRealtimeListening called but Realtime API requires server-side proxy');
            console.log('üí° Using enhanced Audio API instead');
            // No-op since we're using enhanced Audio API fallback
        }

        // Play audio response
        let audioQueue = [];
        let isPlayingAudio = false;
        
        function playAudioDelta(base64Audio) {
            audioQueue.push(base64Audio);
            if (!isPlayingAudio) {
                processAudioQueue();
            }
        }
        
        async function processAudioQueue() {
            if (audioQueue.length === 0) {
                isPlayingAudio = false;
                return;
            }
            
            isPlayingAudio = true;
            setPersonaState('speaking', 'Speaking...');
            
            const base64Audio = audioQueue.shift();
            const audioData = base64ToArrayBuffer(base64Audio);
            const audioBuffer = await audioContext.decodeAudioData(audioData);
            
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContext.destination);
            
            source.onended = () => {
                processAudioQueue();
            };
            
            source.start();
        }

        // Utility functions
        function arrayBufferToBase64(buffer) {
            const bytes = new Uint8Array(buffer);
            let binary = '';
            for (let i = 0; i < bytes.byteLength; i++) {
                binary += String.fromCharCode(bytes[i]);
            }
            return btoa(binary);
        }
        
        function base64ToArrayBuffer(base64) {
            const binaryString = atob(base64);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }



        // --- Enhanced State Management ---
        function setPersonaState(state, text) {
            personaOrb.className = 'orb'; // Reset classes
            personaOrb.classList.add(state);
            statusText.textContent = text ? text.toUpperCase() : '';
            
            // Hide touch indicator after first interaction
            if (state !== 'idle' || text !== 'CONNECTING...') {
                touchIndicator.style.display = 'none';
            }
        }



        // Connect to voice system (Enhanced Audio API for browser compatibility)
        async function connectRealtime() {
            try {
                console.log('üîÑ Initializing voice assistant...');
                setPersonaState('idle', 'CONNECTING...');
                
                // Test Realtime API access (for future server-side implementation)
                const availableModel = await testRealtimeAccess();
                if (availableModel) {
                    console.log('‚úÖ Realtime API available but requires server-side proxy for browser auth');
                    console.log('üöÄ Using enhanced Audio API for optimal browser compatibility');
                } else {
                    console.log('üì± Using enhanced Audio API (Whisper + Assistant + TTS)');
                }
                
                // Use enhanced Audio API for reliable browser-based voice interaction
                await connectEnhancedAudio();
                
            } catch (error) {
                console.error('‚ùå Voice system initialization failed:', error);
                await connectEnhancedAudio();
            }
        }

        // Enhanced touch/click interaction for iPad with voice system
        let isActivated = false;
        
        async function activateAssistant() {
            if (!isActivated) {
                isActivated = true;
                touchIndicator.textContent = 'ACTIVATING...';
                touchIndicator.classList.remove('pulse-hint');
                
                setPersonaState('idle', 'INITIALIZING...');
                
                // Initialize audio and connect to voice system
                const audioInitialized = await initializeAudio();
                if (audioInitialized) {
                    await connectRealtime();
                    setTimeout(() => {
                        touchIndicator.style.display = 'none';
                    }, 2000);
                } else {
                    setPersonaState('idle', 'MICROPHONE ERROR');
                }
            }
        }
        
        // Manual voice recording trigger (backup for voice detection)
        async function startManualRecording() {
            if (!isConnected || isRecording || isSpeaking) return;
            
            console.log('üé§ Manual recording started - speak now!');
            isRecording = true;
            setPersonaState('listening', 'Listening...');
            
            recordingChunks = [];
            mediaRecorder.start();
            
            // Auto-stop after 10 seconds max
            setTimeout(() => {
                if (isRecording) {
                    console.log('‚è∞ Auto-stopping recording after 10 seconds');
                    isRecording = false;
                    setPersonaState('thinking', 'Processing...');
                    mediaRecorder.stop();
                }
            }, 10000);
        }
        
        // Add multiple event listeners for better iPad compatibility
        document.body.addEventListener('click', activateAssistant, { once: true });
        document.body.addEventListener('touchstart', activateAssistant, { once: true });
        
        // Orb click handlers - activate first, then manual recording
        personaOrb.addEventListener('click', (e) => {
            if (!isActivated) {
                activateAssistant();
            } else {
                startManualRecording();
            }
        });
        
        personaOrb.addEventListener('touchstart', (e) => {
            if (!isActivated) {
                activateAssistant();
            } else {
                startManualRecording();
            }
        });

        // Keyboard shortcut for testing (spacebar to record)
        document.addEventListener('keydown', (e) => {
            if (e.code === 'Space' && isActivated && !isRecording && !isSpeaking) {
                e.preventDefault();
                console.log('‚å®Ô∏è Spacebar pressed - starting manual recording');
                startManualRecording();
            }
        });

        // Initial state - wait for user activation
        setPersonaState('idle', 'TAP TO START');

    </script>
</body>
</html>

